{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scientific Paper Analysis System - RAG Implementation  \n",
        "**Version:** 1.0  \n",
        "**Author:** Lorena Melo </br>\n",
        "**Last Updated:** 17 Feb 2025\n",
        "\n",
        "---\n",
        "\n",
        "## üìÑ Overview  \n",
        "This Jupyter Notebook provides an end-to-end Retrieval-Augmented Generation (RAG) system specifically designed for analyzing scientific papers. It enables users to:  \n",
        "\n",
        "- Process PDF academic papers with specialized preprocessing  \n",
        "- Create semantic search capabilities over documents  \n",
        "- Ask natural language questions about paper content  \n",
        "- Receive answers with citations from source material  \n",
        "\n",
        "**Key Technologies Used**:  \n",
        "- OpenAI GPT-4 & Embeddings API  \n",
        "- ChromaDB vector database  \n",
        "- Advanced text processing pipelines  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Critical Requirements  \n",
        "Before using this system, users **MUST**:  \n",
        "\n",
        "1. **Obtain OpenAI API Key**  \n",
        "   - Create account at [OpenAI Platform](https://platform.openai.com/)  \n",
        "   - Generate API key in `API Keys` section  \n",
        "   - **Important**: Add payment method - API usage incurs costs  \n",
        "\n",
        "2. **Understand Costs**  \n",
        "   |      Service  | Estimated Cost $ |  \n",
        "   |---------------|----------------|  \n",
        "   | Embeddings | 0,00013 / 1k tokens |  \n",
        "   | GPT-4 | 0.03/1k tokens input |  \n",
        "\n",
        "   *Costs based on OpenAI pricing as of July 2024*  \n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Features  \n",
        "\n",
        "### 1. PDF Processing Engine  \n",
        "- Header/footer removal  \n",
        "- DOI detection and filtering  \n",
        "- Academic structure preservation  \n",
        "\n",
        "### 2. Vector Database  \n",
        "- ChromaDB persistent storage  \n",
        "- `text-embedding-3-large` embeddings (3072-dimension)  \n",
        "- MMR search algorithm  \n",
        "\n",
        "### 3. Question Answering  \n",
        "- GPT-4 Turbo integration  \n",
        "- Technical term handling  \n",
        "- Page/section citation system  \n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Installation  \n",
        "\n",
        "### Requirements  \n",
        "- Python 3.10+  \n",
        "- Libraries:  \n",
        "```bash\n",
        "pip install langchain langchain-community chromadb pypdf python-dotenv tiktoken\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Configuration  \n",
        "\n",
        "1. **Environment Setup**  \n",
        "   Create `.env` file:  \n",
        "   ```ini\n",
        "   OPENAI_API_KEY=sk-your-key-here\n",
        "   ```\n",
        "\n",
        "2. **Document Preparation**  \n",
        "   - Place PDF in specified path:  \n",
        "   ```python\n",
        "   DOCUMENT_PATH = \"/path/to/your/paper.pdf\"\n",
        "   ```\n",
        "\n",
        "3. **System Settings** (Optional)  \n",
        "   ```python\n",
        "   class Config:\n",
        "       CHUNK_SIZE = 1500  # Optimal for technical content\n",
        "       MODEL_NAME = \"gpt-4-1106-preview\"  # Default model\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "## üíª Usage  \n",
        "\n",
        "### Basic Workflow  \n",
        "1. **First Run**  \n",
        "   ```bash\n",
        "   python scientific_rag.py\n",
        "   ```  \n",
        "   - System will:  \n",
        "     - Process PDF (~2-10 mins depending on paper size)  \n",
        "     - Create ChromaDB vector store  \n",
        "\n",
        "2. **Query Interface**  \n",
        "   ```bash\n",
        "   Sistema de An√°lise Cient√≠fica Pronto. Fa√ßa sua pergunta sobre o artigo:\n",
        "   \n",
        "   Pergunta: [Enter your question]\n",
        "   ```\n",
        "\n",
        "### Example Session  \n",
        "```bash\n",
        "Pergunta: What methodology did the authors use for evaluation?\n",
        "\n",
        "Resposta:\n",
        "The authors employed a hybrid evaluation approach combining:\n",
        "- Quantitative metrics (F1-score: 92.4%, Table 3, page 12)\n",
        "- Human expert validation (Section 4.2, page 8)\n",
        "- Comparative analysis against baseline models (Figure 5, page 14)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Technical Notes  \n",
        "\n",
        "### Cost Management Tips  \n",
        "1. Monitor usage at [OpenAI Usage Dashboard](https://platform.openai.com/usage)  \n",
        "2. For large papers, estimate costs running a cell right after the loading document function.\n",
        "\n",
        "\n",
        "\n",
        "### Model Customization  \n",
        "Available OpenAI Models:  \n",
        "```python\n",
        "# In Config class:\n",
        "MODEL_NAME = \"gpt-4-turbo\"  # Alternative\n",
        "# MODEL_NAME = \"gpt-3.5-turbo\"  # Cheaper but less accurate\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìú Support & Disclaimer  \n",
        "\n",
        "### Support Includes  \n",
        "- Code functionality  \n",
        "- Configuration assistance  \n",
        "- Basic usage guidance  \n",
        "\n",
        "### Not Included  \n",
        "- OpenAI account management  \n",
        "- API cost optimization  \n",
        "- Custom feature development  \n",
        "\n",
        "### Legal Disclaimer  \n",
        "```text\n",
        "This product is dependent on third-party services (OpenAI).\n",
        "The developer is not responsible for:\n",
        "- API service interruptions\n",
        "- Content generated by AI models\n",
        "- Financial charges from API usage\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üì¨ Contact  \n",
        "For support: [lorenamelo.engr@gmail.com]  \n",
        "Report issues: [GitHub Repository Link]  \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2psAfX8RUHW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "laumErOwXeDA"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"RAG System for Scientific Papers.ipynb\"\"\"\n",
        "\n",
        "# Instala√ß√£o de depend√™ncias\n",
        "!pip install -qU langchain langchain-community langchain-openai chromadb pypdf python-dotenv tiktoken\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Configura√ß√£o de ambiente\n",
        "load_dotenv()\n",
        "\n",
        "# Configura√ß√µes do usu√°rio\n",
        "class Config:\n",
        "    OPENAI_API_KEY = \"YOUR_OPENAI_API_KEI\" #os.getenv(\"OPENAI_API_KEY\")\n",
        "    DOCUMENT_PATH = \"/content/2007.03051v1.pdf\"  # Altere\n",
        "    CHROMA_DIR = \"./chroma_db_scientific\"\n",
        "    CHUNK_SIZE = 1500  # Ideal para artigos cient√≠ficos\n",
        "    CHUNK_OVERLAP = 300\n",
        "    MODEL_NAME = \"gpt-4-1106-preview\"  # Mais adequado para an√°lise cient√≠fica\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Verifica√ß√£o inicial\n",
        "if not os.path.exists(config.DOCUMENT_PATH):\n",
        "    raise FileNotFoundError(f\"Documento n√£o encontrado em: {config.DOCUMENT_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YGXFr9p3Xtse"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_scientific_paper(file_path: str):\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Debugging: print how many documents were loaded and a preview of the first document\n",
        "    print(f\"Loaded {len(documents)} documents.\")\n",
        "    if documents:\n",
        "        print(\"Preview of first document:\")\n",
        "        print(documents[0].page_content[:500])\n",
        "\n",
        "    # Preprocess the document by filtering out only obvious non-useful lines\n",
        "    for doc in documents:\n",
        "        lines = doc.page_content.split(\"\\n\")\n",
        "        filtered_lines = []\n",
        "        for line in lines:\n",
        "            # Remove lines that are only digits (likely page numbers)\n",
        "            #if line.strip().isdigit():\n",
        "                #continue\n",
        "            # Remove lines with DOI links if you consider them non-essential\n",
        "            #if \"https://doi.org\" in line:\n",
        "                #continue\n",
        "            filtered_lines.append(line)\n",
        "        doc.page_content = \"\\n\".join(filtered_lines)\n",
        "    return documents\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v1pDD585X1UQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_scientific_paper(config.DOCUMENT_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ougCkJHZZZ3g",
        "outputId": "3e5235c1-1b21-444f-fd32-d09484094f30"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 11 documents.\n",
            "Preview of first document:\n",
            "Carbontracker: Tracking and Predicting the Carbon Footprint of Training\n",
            "Deep Learning Models\n",
            "Lasse F. Wolff Anthony‚àó1 Benjamin Kanding‚àó1 Raghavendra Selvan1\n",
            "Abstract\n",
            "Deep learning (DL) can achieve impressive\n",
            "results across a wide variety of tasks, but\n",
            "this often comes at the cost of training\n",
            "models for extensive periods on specialized\n",
            "hardware accelerators. This energy-intensive\n",
            "workload has seen immense growth in recent\n",
            "years. Machine learning (ML) may become\n",
            "a signiÔ¨Åcant contributor to climate\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Carbontracker: Tracking and Predicting the Carbon Footprint of Training\\nDeep Learning Models\\nLasse F. Wolff Anthony‚àó1 Benjamin Kanding‚àó1 Raghavendra Selvan1\\nAbstract\\nDeep learning (DL) can achieve impressive\\nresults across a wide variety of tasks, but\\nthis often comes at the cost of training\\nmodels for extensive periods on specialized\\nhardware accelerators. This energy-intensive\\nworkload has seen immense growth in recent\\nyears. Machine learning (ML) may become\\na signiÔ¨Åcant contributor to climate change\\nif this exponential trend continues. If practi-\\ntioners are aware of their energy and carbon\\nfootprint, then they may actively take steps to\\nreduce it whenever possible. In this work, we\\npresent carbontracker, a tool for tracking and\\npredicting the energy and carbon footprint of\\ntraining DL models. We propose that energy\\nand carbon footprint of model development\\nand training is reported alongside perfor-\\nmance metrics using tools like carbontracker.\\nWe hope this will promote responsible\\ncomputing in ML and encourage research\\ninto energy-efÔ¨Åcient deep neural networks.1\\n1. Introduction\\nThe popularity of solving problems using deep learn-\\ning (DL) has rapidly increased and with it the need\\nfor ever more powerful models. These models achieve\\nimpressive results across a wide variety of tasks such as\\ngameplay, where AlphaStar reached the highest rank in\\nthe strategy game Starcraft II (Vinyals et al., 2019) and\\nAgent57 surpassed human performance in all 57 Atari\\n2600 games (Badia et al., 2020). This comes at the cost\\nof training the model for thousands of hours on special-\\n*Equal contribution 1Department of Computer Sci-\\nence, University of Copenhagen, Copenhagen, Denmark.\\nCorrespondence to: Lasse F. Wolff Anthony <lassewolffan-\\nthony@gmail.com>, Benjamin Kanding <bmk1212@live.dk>.\\nICML Workshop on \"Challenges in Deploying and monitor-\\ning Machine Learning Systems\", 2020.\\n1Source code for carbontracker is available here:\\nhttps://github.com/lfwa/carbontracker\\nized hardware accelerators such as graphics processing\\nunits (GPUs). From 2012 to 2018 the compute needed\\nfor DL grew300000-fold (Amodei & Hernandez, 2018).\\nThis immense growth in required compute has a\\nhigh energy demand, which in turn increases the\\ndemand for energy production. In 2010 energy\\nproduction was responsible for approximately35% of\\ntotal anthropogenic greenhouse gas (GHG) emissions\\n(Bruckner et al., 2014). Should this exponential trend in\\nDL compute continue then machine learning (ML) may\\nbecome a signiÔ¨Åcant contributor to climate change.\\nThis can be mitigated by exploring how to improve\\nenergy efÔ¨Åciency in DL. Moreover, if practitioners are\\naware of their energy and carbon footprint, then they\\nmay actively take steps to reduce it whenever possible.\\nWe show that in ML, these can be simple steps that\\nresult in considerable reductions to carbon emissions.\\nThe environmental impact of ML in research and\\nindustry has seen increasing interest in the last year\\nfollowing the 2018 IPCC special report (IPCC, 2018)\\ncalling for urgent action in order to limit global\\nwarming to 1.5 ‚ó¶C. We brieÔ¨Çy review some notable\\nwork on the topic. Strubell et al. (2019) estimated the\\nÔ¨Ånancial and environmental costs of R&D and hyper-\\nparameter tuning for various state-of-the-art (SOTA)\\nneural network (NN) models in natural language\\nprocessing (NLP). They point out that increasing cost\\nand emissions of SOTA models contribute to a lack\\nof equity between those researchers who have access\\nto large-scale compute, and those who do not. The\\nauthors recommend that metrics such as training time,\\ncomputational resources required, and model sensi-\\ntivity to hyperparameters should be reported to enable\\ndirect comparison between models. Lacoste et al. (2019)\\nprovided the Machine Learning Emissions Calculatorthat\\nrelies on self-reporting. The tool can estimate the car-\\nbon footprint of GPU compute by specifying hardware\\ntype, hours used, cloud provider, and region. Hender-\\nson et al. (2020) presented theexperiment-impact-tracker\\nframework and gave various strategies for mitigating\\ncarbon emissions in ML. Their Python framework\\nallows for estimating the energy and carbon impact\\narXiv:2007.03051v1  [cs.CY]  6 Jul 2020'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='Carbontracker\\nof ML systems as well as the generation of ‚ÄúCarbon\\nImpact Statements‚Äù for standardized reporting hereof.\\nIn this work, we propose carbontracker, a tool for\\ntracking and predicting the energy consumption\\nand carbon emissions of training DL models. The\\nmethodology is similar to that of Henderson et al.\\n(2020) but differs from prior art in two major ways:\\n(1) We allow for a further proactive and intervention-\\ndriven approach to reducing carbon emissions\\nby supporting predictions. Model training can be\\nstopped, at the user‚Äôs discretion, if the predicted\\nenvironmental cost is exceeded.\\n(2) We support a variety of different environments\\nand platforms such as clusters, desktop comput-\\ners, and Google Colab notebooks, allowing for a\\nplug-and-play experience.\\nWe experimentally evaluate the tool on several different\\ndeep convolutional neural network (CNN) architec-\\ntures and datasets for medical image segmentation\\nand assess the accuracy of its predictions. We present\\nconcrete recommendations on how to reduce carbon\\nemissions considerably when training DL models.\\n2. Design and Implementation\\nThe design philosophy that guided the development\\nof carbontracker can be summarized by the following\\nprinciples:\\nPythonic The majority of ML takes place in the Python\\nlanguage (Wilcox et al., 2017). We want the tool to\\nbe as easy as possible to integrate into existing work\\nenvironments making Python the language of choice.\\nUsable The required effort and added code must be\\nminimal and not obfuscate the existing code structure.\\nExtensible Adding and maintaining support for\\nchanging application programming interfaces (APIs)\\nand new hardware should be straightforward.\\nFlexible The user should have full control over what\\nis monitored and how this monitoring is performed.\\nPerformance The performance impact of using the\\ntool must be negligible, and computation should be\\nminimal. It must not affect training.\\nInterpretable Carbon footprint expressed ingCO2eq\\nis often meaningless. A common understanding of the\\nimpact should be facilitated through conversions.\\nCarbontracker is an open-source tool written in Python\\nfor tracking and predicting the energy consumption\\nand carbon emissions of training DL models. It is avail-\\nable through the Python Package Index (PyPi). The tool\\nis implemented as a multithreaded program. It utilizes\\nseparate threads to collect power measurements and\\nfetch carbon intensity in real-time for parallel efÔ¨Åciency\\nand to not disrupt the model training in the main thread.\\nAppendix A has further implementation details.\\nCarbontracker supports predicting the total duration,\\nenergy, and carbon footprint of training a DL model.\\nThese predictions are based on a user-speciÔ¨Åed number\\nof monitored epochs with a default of 1. We forecast\\nthe carbon intensity of electricity production during\\nthe predicted duration using the supported APIs. The\\nforecasted carbon intensity is then used to predict the\\ncarbon footprint. Following our preliminary research,\\nwe use a simple linear model for predictions.\\n3. Experiments and Results\\nIn order to evaluate the performance and behavior\\nof carbontracker, we conducted experiments on three\\nmedical image datasets using two different CNN\\nmodels: U-net (Ronneberger et al., 2015) and lungVAE\\n(Selvan et al., 2020). The models were trained for\\nthe task of medical image segmentation using three\\ndatasets: DRIVE (Staal et al., 2004), LIDC (Armato III\\net al., 2004), and CXR (Jaeger et al., 2014). Details on\\nthe models and datasets are given in Appendix B. All\\nmeasurements were taken usingcarbontracker version\\n1.1.2. We performed our experiments on a single\\nNVIDIA TITAN RTX GPU with 12 GB memory and\\ntwo Intel central processing units (CPUs).\\nIn line with our message of reporting energy and\\ncarbon footprint, we used carbontracker to generate\\nthe following statement: The training of models in\\nthis work is estimated to use37.445 kWhof electricity\\ncontributing to 3.166 kgof CO2eq. This is equivalent\\nto 26.296 kmtravelled by car (see A.5).\\nAn overview of predictions fromcarbontracker based on\\nmonitoring for 1 training epoch for the trained models\\ncompared to the measured values is shown in Figure 1.\\nThe errors in the energy predictions are4.9‚Äì19.1% com-\\npared to the measured energy values,7.3‚Äì19.9% for the\\nCO2eq, and 0.8‚Äì4.6% for the duration. The error in the\\nCO2eq predictions are also affected by the quality of the\\nforecasted carbon intensity from the APIs used bycar-\\nbontracker. This is highlighted in Figure 2, which shows\\nthe estimated carbon emissions (gCO2eq) of training\\nour U-net model on LIDC in Denmark and Great\\nBritain for different carbon intensity estimation meth-\\nods. As also shown by Henderson et al. (2020), we see\\nthat using country or region-wide average estimates'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='Carbontracker\\nFigure 1.Comparison of predicted and measured values of\\nenergy in kWh (left), emissions ingCO2eq (center), and du-\\nration in s (right) for the full training session when predicting\\nafter a single epoch. The diagonal line represents predic-\\ntions that are equal to the actual measured consumption.\\nDescription of the models and datasets are in Appendix B.\\nFigure 2.Carbon emissions (gCO2eq) of training the U-net on\\nLIDC dataset for different carbon intensity estimation meth-\\nods. (left) The emissions of training in Denmark and (right)\\nin Great Britain at 2020-05-21 22:00 local time. Real-time indi-\\ncates that the current intensity is fetched every15 minduring\\ntraining using the APIs supported by carbontracker. The\\naverage intensities are from 2016 (see Figure 8 in Appendix).\\nmay severely overestimate (or under different circum-\\nstances underestimate) emissions. This illustrates the\\nimportance of using real-time (or forecasted) carbon\\nintensity for accurate estimates of carbon footprint.\\nFigure 3 summarizes the relative energy consumption\\nof each component across all runs. We see that\\nwhile the GPU uses the majority of the total energy,\\naround 50‚Äì60%, the CPU and dynamic random-access\\nmemory (DRAM) also account for a signiÔ¨Åcant part\\nof the total consumption. This is consistent with the\\nÔ¨Åndings of Gorkovenko & Dholakia (2020), who found\\nthat GPUs are responsible for around 70% of power\\nconsumption, CPU for 15%, and RAM for 10% when\\ntesting on the TensorFlow benchmarking suite for DL\\non Lenovo ThinkSystem SR670 servers. As such, only\\naccounting for GPU consumption when quantifying\\nthe energy and carbon footprint of DL models will lead\\nto considerable underestimation of the actual footprint.\\n4. Reducing Your Carbon Footprint\\nThe carbon emissions that occur when training DL\\nmodels are not irreducible and do not have to simply\\nbe the cost of progress within DL. Several steps can\\nFigure 3.Comparison of energy usage by component shown\\nas the relative energy usage (%) out of the total energy spent\\nduring training. We see that the GPU uses the majority of the\\nenergy, about50‚Äì60%, but the CPU and DRAM also account\\nfor a signiÔ¨Åcant amount of the total energy consumption\\nacross all models and datasets.\\nFigure 4.Estimated carbon emissions (gCO2eq) of training\\nour models (see Appendix B) in different EU-28 countries.\\nThe calculations are based on the average carbon intensities\\nfrom 2016 (see Figure 8 in Appendix).\\nbe taken in order to reduce this footprint considerably.\\nIn this section, we outline some strategies for practi-\\ntioners to directly mitigate their carbon footprint when\\ntraining DL models.\\nLow Carbon Intensity Regions The carbon intensity\\nof electricity production varies by region and is\\ndependent on the energy sources that power the local\\nelectrical grid. Figure 4 illustrates how the variation\\nin carbon intensity between regions can inÔ¨Çuence the\\ncarbon footprint of training DL models. Based on the\\n2016 average intensities, we see that a model trained\\nin Estonia may emit more than 61 times theCO2eq as\\nan equivalent model would when trained in Sweden.\\nIn perspective, our U-net model trained on the LIDC\\ndataset would emit 17.7 gCO2eq or equivalently the\\nsame as traveling 0.14 kmby car when trained in\\nSweden. However, training in Estonia it would emit\\n1087.9 gCO2eq or the same as traveling9.04 kmby car\\nfor just a single training session.\\nAs training DL models is generally not latency bound,\\nwe recommend that ML practitioners move training\\nto regions with a low carbon intensity whenever it is\\npossible to do so. We must further emphasize that for'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='Carbontracker\\nFigure 5.Real-time carbon intensity ( gCO2eq/kWh) for\\nDenmark (DK) and Great Britain (GB) from 2020-05-18 to\\n2020-05-25 shown in local time. The data is collected using\\nthe APIs supported bycarbontracker. The carbon intensities\\nare volatile to changes in energy demand and depend on the\\nenergy sources available.\\nlarge-scale models that are trained on multiple GPUs\\nfor long periods, such as OpenAI‚Äôs GPT-3 language\\nmodel (Brown et al., 2020), it is imperative that training\\ntakes place in low carbon intensity regions in order\\nto avoid several megagrams of carbon emissions.\\nThe absolute difference in emissions may even be\\nsigniÔ¨Åcant between two green regions, like Sweden\\nand France, for such large-scale runs.\\nTraining Times The time period in which a DL model\\nis trained affects its overall carbon footprint. This\\nis caused by carbon intensity changing throughout\\nthe day as energy demand and capacity of energy\\nsources change. Figure 5 shows the carbon intensity\\n(gCO2eq/kWh) for Denmark and Great Britain in the\\nweek of 2020-05-18 to 2020-05-25 collected with the\\nAPIs supported bycarbontracker. A model trained dur-\\ning low carbon intensity hours of the day in Denmark\\nmay emit as little as 1\\n4 the CO2eq of one trained during\\npeak hours. A similar trend can be seen for Great\\nBritain, where 2-fold savings in emissions can be had.\\nWe suggest that ML practitioners shift training to take\\nplace in low carbon intensity time periods whenever\\npossible. The time period should be determined on a\\nregional level.\\nEfÔ¨Åcient Algorithms The use of efÔ¨Åcient algorithms\\nwhen training DL models can further help reduce\\ncompute-resources and thereby also carbon emissions.\\nHyperparameter tuning may be improved by substitut-\\ning grid search for random search (Bergstra & Bengio,\\n2012), using Bayesian optimization (Snoek et al., 2012)\\nor other optimization techniques like Hyperband (Li\\net al., 2017). Energy efÔ¨Åciency of inference in deep\\nneural networks (DNNs) is also an active area of\\nresearch with methods such as quantization aware\\ntraining, energy-aware pruning (Yang et al., 2017),\\nand power- and memory-constrained hyperparameter\\noptimization like HyperPower (Stamoulis et al., 2018).\\nEfÔ¨Åcient Hardware and Settings Choosing more\\nenergy-efÔ¨Åcient computing hardware and settings may\\nalso contribute to reducing carbon emissions. Some\\nGPUs have substantially higher efÔ¨Åciency in terms\\nof Ô¨Çoating point operations per second (FLOPS) per\\nwatt of power usage compared to others (Lacoste et al.,\\n2019). Power management techniques like dynamic\\nvoltage and frequency scaling (DVFS) can further help\\nconserve energy consumption (Li et al., 2016) and for\\nsome models even reduce time to reach convergence\\n(Tang et al., 2019). Tang et al. (2019) show that DVFS\\ncan be applied to GPUs to help conserve about8.7% to\\n23.1% energy consumption for training different DNNs\\nand about 19.6% to 26.4% for inference. Moreover, the\\nauthors show that the default frequency settings on\\ntested GPUs, such as NVIDIA‚Äôs Pascal P100 and Volta\\nV100, are often not optimized for energy efÔ¨Åciency in\\nDNN training and inference.\\n5. Discussion and Conclusion\\nThe current trend in DL is a rapidly increasing demand\\nfor compute that does not appear to slow down.\\nThis is evident in recent models such as the GPT-3\\nlanguage model (Brown et al., 2020) with 175 billion\\nparameters requiring an estimated28000 GPU-days to\\ntrain excluding R&D (see Appendix D). We hope to\\nspread awareness about the environmental impact of\\nthis increasing compute through accurate reporting\\nwith the use of tools such as carbontracker. Once\\ninformed, concrete and often simple steps can be taken\\nin order to reduce the impact.\\nSOTA-results in DL are frequently determined by a\\nmodel‚Äôs performance through metrics such as accuracy,\\nAUC score, or similar performance metrics. Energy-\\nefÔ¨Åciency is usually not one of these. While such\\nperformance metrics remain a crucial measure of model\\nsuccess, we hope to promote an increasing focus on\\nenergy-efÔ¨Åciency. We must emphasize that we do not\\nargue that compute-intensive research is not essential\\nfor the progress of DL. We believe, however, that the im-\\npact of this compute should be minimized. We propose\\nthat the total energy and carbon footprint of model de-\\nvelopment and training is reported alongside accuracy\\nand similar metrics to promote responsible computing\\nin ML and research into energy-efÔ¨Åcient DNNs.\\nIn this work, we showed that ML risks becoming a\\nsigniÔ¨Åcant contributor to climate change. To this end,\\nwe introduced the open-source carbontracker tool for\\ntracking and predicting the total energy consumption\\nand carbon emissions of training DL models. This\\nenables practitioners to be aware of their footprint and\\ntake action to reduce it.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='Carbontracker\\nACKNOWLEDGEMENTS\\nThe authors would like to thank Morten Pol Engell-\\nN√∏rreg√•rd for the thorough feedback on the thesis\\nversion of this work. The authors also thank the\\nanonymous reviewers and early users ofcarbontracker\\nfor their insightful feedback.\\nReferences\\nAmodei, D. and Hernandez, D. Ai and compute.Herun-\\ntergeladen von https://blog. openai. com/aiand-compute,\\n2018.\\nArmato III, S. G., McLennan, G., McNitt-Gray,\\nM. F., Meyer, C. R., Yankelevitz, D., Aberle, D. R.,\\nHenschke, C. I., Hoffman, E. A., Kazerooni, E. A.,\\nMacMahon, H., et al. Lung image database consor-\\ntium: developing a resource for the medical imaging\\nresearch community.Radiology, 232(3):739‚Äì748, 2004.\\nAscierto, R. Uptime Institute 2018 Data Center Survey.\\nTechnical report, Uptime Institute, 2018.\\nAscierto, R. Uptime Institute 2019 Data Center Survey.\\nTechnical report, Uptime Institute, 2019.\\nAvelar, V ., Azevedo, D., and French, A. PUE‚Ñ¢ : A Com-\\nprehensive Examination of the Metric. GreenGrid,\\npp. 1‚Äì83, 2012.\\nBadia, A. P ., Piot, B., Kapturowski, S., Sprechmann,\\nP ., Vitvitskyi, A., Guo, D., and Blundell, C. Agent57:\\nOutperforming the atari human benchmark, 2020.\\nBergstra, J. and Bengio, Y. Random search for hyper-\\nparameter optimization. Journal of Machine Learning\\nResearch, 13:281‚Äì305, 2012. ISSN 15324435.\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P ., Neelakantan, A., Shyam, P ., Sastry,\\nG., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,\\nG., Henighan, T., Child, R., Ramesh, A., Ziegler,\\nD. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler,\\nE., Litwin, M., Gray, S., Chess, B., Clark, J., Berner,\\nC., McCandlish, S., Radford, A., Sutskever, I., and\\nAmodei, D. Language models are few-shot learners,\\n2020.\\nBruckner, T., Bashmakov, Y., Mulugetta, H., and\\nChum, A. 2014: Energy systems. In Climate Change\\n2014: Mitigation of Climate Change. Contribution of\\nWorking Group III to the Fifth Assessment Report of the\\nIntergovernmental Panel on Climate Change. 2014.\\nDavid, H., Gorbatov, E., Hanebutte, U. R., Khanna, R.,\\nand Le, C. RAPL: Memory power estimation and\\ncapping. In Proceedings of the International Symposium\\non Low Power Electronics and Design, pp. 189‚Äì194, 2010.\\nISBN 9781450301466. doi: 10.1145/1840845.1840883.\\nGoodward, J. and Kelly, A. Bottom line on offsets.\\nTechnical report, The World Resources Institute, 10\\nG Street, NE Suite 800 Washington, D. C . . . , 2010.\\nGorkovenko, M. and Dholakia, A. Towards Power Ef-\\nÔ¨Åciency in Deep Learning on Data Center Hardware.\\npp. 1814‚Äì1820, 2020.\\nHenderson, P ., Hu, J., Romoff, J., Brunskill, E.,\\nJurafsky, D., and Pineau, J. Towards the Sys-\\ntematic Reporting of the Energy and Carbon\\nFootprints of Machine Learning. jan 2020. URL\\nhttp://arxiv.org/abs/2002.05651.\\nIPCC. Global Warming of 1.5 ¬∞C. An IPCC Special\\nReport on the impacts of global warming of 1.5 ¬∞C\\nabove pre-industrial levels and related global\\ngreenhouse gas emission pathways, in the context\\nof strengthening the global response to the threat of\\nclimate change,. Technical report, 2018.\\nJaeger, S., Candemir, S., Antani, S., W√°ng, Y.-X. J.,\\nLu, P .-X., and Thoma, G. Two public chest x-ray\\ndatasets for computer-aided screening of pulmonary\\ndiseases. Quantitative imaging in medicine and surgery,\\n4(6):475, 2014.\\nKingma, D. and Ba, J. Adam optimizer.arXiv preprint\\narXiv:1412.6980, pp. 1‚Äì15, 2014.\\nLacoste, A., Luccioni, A., Schmidt, V ., and Dandres,\\nT. Quantifying the Carbon Emissions of Machine\\nLearning. Technical report, 2019.\\nLi, D., Chen, X., Becchi, M., and Zong, Z. Evaluating\\nthe energy efÔ¨Åciency of deep convolutional neural\\nnetworks on CPUs and GPUs. InProceedings - 2016\\nIEEE International Conferences on Big Data and Cloud\\nComputing, BDCloud 2016, Social Computing and Net-\\nworking, SocialCom 2016 and Sustainable Computing\\nand Communications, SustainCom 2016, pp. 477‚Äì484.\\nInstitute of Electrical and Electronics Engineers\\nInc., oct 2016. ISBN 9781509039364. doi: 10.1109/\\nBDCloud-SocialCom-SustainCom.2016.76.\\nLi, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A.,\\nand Talwalkar, A. Hyperband: A novel bandit-based\\napproach to hyperparameter optimization. J. Mach.\\nLearn. Res., 18(1):6765‚Äì6816, January 2017. ISSN\\n1532-4435.\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury,\\nJ., Chanan, G., Killeen, T., Lin, Z., Gimelshein,\\nN., Antiga, L., Desmaison, A., K√∂pf, A., Yang,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='Carbontracker\\nE., DeVito, Z., Raison, M., Tejani, A., Chil-\\namkurthy, S., Steiner, B., Fang, L., Bai, J., and\\nChintala, S. PyTorch: An Imperative Style, High-\\nPerformance Deep Learning Library. 2019. URL\\nhttp://arxiv.org/abs/1912.01703.\\nRonneberger, O., Fischer, P ., and Brox, T. U-net: Con-\\nvolutional networks for biomedical image segmen-\\ntation. In Lecture Notes in Computer Science (including\\nsubseries Lecture Notes in ArtiÔ¨Åcial Intelligence and\\nLecture Notes in Bioinformatics), volume 9351, pp. 234‚Äì\\n241, may 2015. doi: 10.1007/978-3-319-24574-4_28.\\nURL http://arxiv.org/abs/1505.04597.\\nSelvan, R., Dam, E. B., Detlefsen, N. S., Rischel,\\nS., Sheng, K., Nielsen, M., and Pai, A. Lung\\nSegmentation from Chest X-rays using Variational\\nData Imputation. In ICML Workshop on The Art of\\nLearning with Missing Values , 2020. URL https:\\n//openreview.net/forum?id=dlzQM28tq2W.\\nSnoek, J., Larochelle, H., and Adams, R. P . Practical\\nbayesian optimization of machine learning algo-\\nrithms. In Advances in neural information processing\\nsystems, pp. 2951‚Äì2959, 2012.\\nStaal, J., Abr√†moff, M. D., Niemeijer, M., Viergever,\\nM. A., and Van Ginneken, B. Ridge-based vessel\\nsegmentation in color images of the retina. IEEE\\nTransactions on Medical Imaging, 23(4):501‚Äì509, apr\\n2004. ISSN 02780062. doi: 10.1109/TMI.2004.825627.\\nStamoulis, D., Cai, E., Juan, D. C., and Marculescu,\\nD. HyperPower: Power- and memory-constrained\\nhyper-parameter optimization for neural networks.\\nIn Proceedings of the 2018 Design, Automation and\\nTest in Europe Conference and Exhibition, DATE 2018,\\nvolume 2018-Janua, pp. 19‚Äì24, dec 2018. ISBN\\n9783981926316. doi: 10.23919/DATE.2018.8341973.\\nURL http://arxiv.org/abs/1712.02446.\\nStrubell, E., Ganesh, A., and McCallum, A. Energy and\\nPolicy Considerations for Deep Learning in NLP. pp.\\n3645‚Äì3650, 2019. doi: 10.18653/v1/p19-1355. URL\\nhttps://bit.ly/2JTbGnI.\\nTang, Z., Wang, Y., Wang, Q., and Chu, X. The impact\\nof GPU DVFS on the energy and performance of\\ndeep Learning: An Empirical Study. Ine-Energy 2019\\n- Proceedings of the 10th ACM International Conference\\non Future Energy Systems , pp. 315‚Äì325, may 2019.\\nISBN 9781450366717. doi: 10.1145/3307772.3328315.\\nURL http://arxiv.org/abs/1905.11012.\\nVinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu,\\nM., Dudzik, A., Chung, J., Choi, D. H., Powell, R.,\\nEwalds, T., Georgiev, P ., et al. Grandmaster level in\\nstarcraft ii using multi-agent reinforcement learning.\\nNature, 575(7782):350‚Äì354, 2019.\\nWilcox, M., Schuermans, S., Voskoglou, C., and\\nSobolevski, A. Developer economics: State of the\\ndeveloper nation q1 2017. Technical report, 2017.\\nYang, T. J., Chen, Y. H., and Sze, V . Designing\\nenergy-efÔ¨Åcient convolutional neural networks\\nusing energy-aware pruning. In Proceedings\\n- 30th IEEE Conference on Computer Vision\\nand Pattern Recognition, CVPR 2017 , volume\\n2017-Janua, pp. 6071‚Äì6079, nov 2017. ISBN\\n9781538604571. doi: 10.1109/CVPR.2017.643. URL\\nhttp://arxiv.org/abs/1611.05128.\\nYuventi, J. and Mehdizadeh, R. A critical analysis of\\nPower Usage Effectiveness and its use in commu-\\nnicating data center energy consumption. Energy\\nand Buildings, 64:90‚Äì94, 2013. ISSN 03787788. doi:\\n10.1016/j.enbuild.2013.04.015.\\nA. Implementation details\\nListing 1.Example of the default setup added to training\\nscripts for tracking and predicting withcarbontracker.\\n1 from carbontracker.tracker\\nimport CarbonTracker‚Ü™‚Üí\\n2\\n3\\n4 tracker\\n= CarbonTracker(epochs=<your epochs>)‚Ü™‚Üí\\n5\\n6 for epoch in range(<your epochs>):\\n7 tracker.epoch_start()\\n8\\n9 # Your model training.\\n10\\n11 tracker.epoch_end()\\n12\\n13 tracker.stop()\\nCarbontracker is a multithreaded program. Figure 6\\nillustrates a high-level overview of the program.\\nA.1. On\\nthe Topic of Power and Energy Measurements\\nIn our work, we measure the total power of selected\\ncomponents such as the GPU, CPU, and DRAM. It\\ncan be argued that dynamic power rather than total\\npower would more fairly represent a user‚Äôs power\\nconsumption when using large clusters or cloud\\ncomputing. We argue that these computing resources\\nwould not have to exist if the user did not use them. As'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='Carbontracker\\nListing 2.Example output of usingcarbontracker to track and\\npredict the energy and carbon footprint of training a DL\\nmodel.\\nCarbonTracker: The following components\\nwere found: GPU with device(s) TITAN\\nRTX. CPU with device(s) cpu:0, cpu:1.\\n‚Ü™‚Üí\\n‚Ü™‚Üí\\nCarbonTracker: Carbon intensity\\nfor the next 1:54:54 is predicted to\\nbe 54.09 gCO2/kWh at detected location:\\nCopenhagen, Capital Region, DK.\\n‚Ü™‚Üí\\n‚Ü™‚Üí\\n‚Ü™‚Üí\\nCarbonTracker:\\nPredicted consumption for 100 epoch(s):\\nTime: 1:54:54\\nEnergy: 1.159974 kWh\\nCO2eq: 62.744032 g\\nThis is equivalent to:\\n0.521130 km travelled by car\\nCarbonTracker: Average\\ncarbon intensity during training\\nwas 58.25 gCO2/kWh at detected location:\\nCopenhagen, Capital Region, DK.\\n‚Ü™‚Üí\\n‚Ü™‚Üí\\n‚Ü™‚Üí\\nCarbonTracker:\\nActual consumption for 100 epoch(s):\\nTime: 1:55:55\\nEnergy: 1.334319 kWh\\nCO2eq: 77.724065 g\\nThis is equivalent to:\\n0.645549 km travelled by car\\nCarbonTracker: Finished monitoring.\\nsuch, the user should also be accountable for the static\\npower consumption during the period in which they\\nreserve the resource. It is also a pragmatic solution as\\naccurately estimating dynamic power is challenging\\ndue to the infeasibility of measuring static power by\\nsoftware and the difÔ¨Åculty in storing and updating\\ninformation about static power for a multitude of\\ndifferent components. A similar argument can be made\\nfor the inclusion of life-cycle aspects in our energy\\nestimates, such as accounting for the energy attributed\\nto the manufacturing of system components. Like\\nHenderson et al. (2020), we ignore these aspects due\\nto the difÔ¨Åculties in their estimation.\\nThe power and energy monitoring in carbontracker is\\nlimited to a few main components of computational\\nsystems. Additional power consumed by the support-\\ning infrastructure, such as that used for cooling or\\npower delivery, is accounted for by multiplying the\\nmeasured power by the Power Usage Effectiveness\\n(PUE) of the data center hosting the compute, as sug-\\ngested by Strubell et al. (2019). PUE is a ratio describing\\nthe efÔ¨Åciency of a data center and the energy overhead\\nof the computing equipment. It is deÔ¨Åned as the ratio\\nof the total energy used in a data center facility to the\\nenergy used by the IT equipment such as the compute,\\nstorage, and network equipment (Avelar et al., 2012):\\nPUE= Total Facility Energy\\nIT Equipment Energy. (1)\\nPrevious research has examined PUE and its shortcom-\\nings (Yuventi & Mehdizadeh, 2013). These shortcom-\\nings may largely be resolved by data centers reporting\\nan average PUE instead of a minimum observed value.\\nIn our work, we use a PUE of1.58, the global average\\nfor data centers in 2018 as reported by Ascierto (2018).2\\nThis may lead to inaccurate estimates of power and\\nenergy consumption of compute in energy-efÔ¨Åcient\\ndata centers; e.g., Google reports a Ô¨Çeetwide PUE of\\n1.10 for 2020.3 Future work may, therefore, explore\\nalternatives to using an average PUE value as well\\nas to include more components in our measurements\\nto improve the accuracy of our estimation. Currently,\\nthe software needed to take power measurements for\\ncomponents beyond the GPU, CPU, and DRAM is\\nextremely limited or in most cases, non-existent.\\nA.2. On the Topic of Carbon Offsetting\\nCarbon emissions can be compensated for by carbon\\noffsetting or the purchases of Renewable Energy\\nCredits (RECs). Carbon offsetting is the reduction in\\nemissions made to compensate for emissions occurring\\nelsewhere (Goodward & Kelly, 2010). We ignore such\\noffsets and RECs in our reporting as to encourage\\nresponsible computing in the ML community that\\nwould further reduce global emissions. See Henderson\\net al. (2020) for an extended discussion on carbon\\noffsets and why they also do not account for them\\nin the experiment-impact-tracker framework. Our\\ncarbon footprint estimate is solely based on the energy\\nconsumed during training of DL models.\\nA.3. Power and Energy Tracking\\nThe power and energy tracking in carbontracker\\noccurs in the carbontracker thread. The thread\\ncontinuously collects instantaneous power samples\\nin real-time for every available device of the speciÔ¨Åed\\ncomponents. Once samples for every device has been\\ncollected, the thread will sleep for a Ô¨Åxed interval\\nbefore collecting samples again. When the epoch ends,\\nthe thread stores the epoch duration. Finally, after the\\ntraining loop completes we calculate the total energy\\n2Early versions (v1.1.2 and earlier) ofcarbontracker used\\na PUE of 1.58. This has subsequently been updated to1.67,\\nthe global average for 2019 (Ascierto, 2019).\\n3See https://www.google.com/about/\\ndatacenters/efficiency/'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='Carbontracker\\nFigure 6.A visualization of thecarbontracker control Ô¨Çow. Themain thread instantiates theCarbonTracker class which then\\nspawns the carbontracker and carbonintensity daemon threads. The carbontracker thread continuously collects\\npower measurements for available devices. Thecarbonintensity thread fetches the current carbon intensity every900 s.\\nWhen the speciÔ¨Åed epochs before predicting have passed, the total predicted consumption is reported tostdout and optional\\nlog Ô¨Åles. Similarly, when the speciÔ¨Åed amount of epochs have been monitored, the actual measured consumption is reported,\\nafter which thecarbontracker and carbonintensity threads join themain thread. Finally, thecarbontracker object runs\\nthe cleanup routinedelete() which releases all used resources.\\nconsumption E as\\nE =PUE\\n‚àë\\ne‚ààE\\n‚àë\\nd‚ààD\\nPavg,deTe (2)\\nwhere Pavg,de is the average power consumed by device\\nd‚ààDin epoch e‚ààE, and Te is the duration of epoche.\\nThe components supported by carbontracker in its\\ncurrent form are the GPU, CPU, and DRAM due to the\\naforementioned restrictions. NVIDIA GPUs represent\\na large share of Infrastructure-as-a-Service compute\\ninstance types with dedicated accelerators. So we\\nsupport NVIDIA GPUs as power sampling is exposed\\nthrough the NVIDIA Management Library (NVML)4.\\nLikewise, we support Intel CPUs and DRAM through\\nthe Intel Running Average Power Limit (Intel RAPL)\\ninterface (David et al., 2010).\\n4https://developer.nvidia.com/\\nnvidia-management-library-nvml\\nA.4. Converting\\nEnergy Consumption to Carbon Emissions\\nWe can estimate the carbon emissions resulting from\\nthe electricity production of the energy consumed\\nduring training as the product of the energy and carbon\\nintensity as shown in (3):\\nCarbon Footprint= Energy Consumption√ó\\nCarbon Intensity. (3)\\nThe used carbon intensity heavily inÔ¨Çuences the\\naccuracy of this estimate. Incarbontracker, we support\\nthe fetching of carbon intensity in real-time through\\nexternal APIs.We dynamically determine the loca-\\ntion based on the IP address of the local compute\\nthrough the Python geocoding library geocoder5.\\nUnfortunately, there does not currently exist a globally\\naccurate, free, and publicly available real-time carbon\\nintensity database. This makes determining the carbon\\nintensity to use for the conversion more difÔ¨Åcult.\\n5https://github.com/DenisCarriere/\\ngeocoder'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='Carbontracker\\nWe solve this problem by using several APIs that are\\nlocal to each region. It is currently limited to Denmark\\nand Great Britain. Other regions default to an average\\ncarbon intensity for the EU-28 countries in 20176. For\\nDenmark we use data from Energi Data Service7 and\\nfor Great Britain we use the Carbon Intensity API8.\\nA.5. Logging\\nFinally,carbontracker has extensive logging capabilities\\nenabling transparency of measurements and enhanc-\\ning the reproducibility of experiments. The user may\\nspecify the desired path for these log Ô¨Åles. We use the\\nlogging API9 provided by the standard Python library.\\nAdditional functionality for interaction\\nwith logs has also been added through the\\ncarbontracker.parser module. Logs may\\neasily be parsed into Python dictionaries containing all\\ninformation regarding the training sessions, including\\npower and energy usages, epoch durations, devices\\nmonitored, whether the model stopped early, and the\\noutputted prediction. We further support aggregating\\nlogs into a single estimate of the total impact of all\\ntraining sessions. By using different log directories, the\\nuser can easily keep track of the total impact of each\\nmodel trained and developed. The user may then use\\nthe provided parser functionality to estimate the full\\nimpact of R&D.\\nCurrent version of carbontracker uses kilometers trav-\\nelled by car as the carbon emissions conversion. This\\ndata is retrieved from the averageCO2eq emissions of\\na newly registered car in the European Union in 201810.\\nB. Models and Data\\nIn our experimental evaluation, we trained two CNN\\nmodels on three medical image datasets for the task\\nof image segmentation. The models were developed\\nin PyTorch (Paszke et al., 2019). We describe each of\\nthe models and datasets in turn below.\\nU-net DRIVE This is a standard U-net model (Ron-\\nneberger et al., 2015) trained on the DRIVE dataset\\n6https://www.eea.europa.\\neu/data-and-maps/data/\\nco2-intensity-of-electricity-generation\\n7https://energidataservice.dk/\\n8https://carbonintensity.org.uk/\\n9https://docs.python.org/3/library/\\nlogging.html\\n10https://www.eea.europa.\\neu/data-and-maps/indicators/\\naverage-co2-emissions-from-motor-vehicles/\\nassessment-1\\n(Staal et al., 2004). DRIVE stands for Digital Retinal\\nImages for Vessel Extraction and is intended for\\nsegmentation of blood vessels in retinal images. The\\nimages are 768 by 584 pixels and JPEG compressed.\\nWe used a training set of15 images and trained for300\\nepochs with a batch size of4 and a learning rate of10‚àí3\\nwith the Adam optimizer (Kingma & Ba, 2014).\\nU-net CXR The model is based on a U-net (Ron-\\nneberger et al., 2015) with slightly changed parameters.\\nThe dataset comprises of chest X-rays (CXR) with lung\\nmasks curated for pulmonary tuberculosis detection\\n(Jaeger et al., 2014). We use528 CXRs for training and\\n176 for validation without any data augmentation. We\\ntrained the model for 200 epochs with a batch size of\\n12, a learning rate of 10‚àí4, and weight decay of 10‚àí5\\nwith the Adam optimizer (Kingma & Ba, 2014).\\nU-net LIDC This is also a standard U-net model\\n(Ronneberger et al., 2015) but trained on a preprocessed\\nLIDC-IDRI dataset (Armato III et al., 2004) 11. The\\nLIDC-IDRI dataset consists of 1018 thoracic computed\\ntomography (CT) scans with annotated lesions from\\nfour different radiologists. We trained our model on\\nthe annotations of a single radiologist for100 epochs.\\nWe used a batch size of64 and a learning rate of10‚àí3\\nwith the Adam optimizer (Kingma & Ba, 2014).\\nlungV AE CXRThis model and dataset is from the\\nopen source model available from Selvan et al. (2020).\\nThe model uses a U-net type segmentation network\\nand a variational encoder for data imputation. The\\ndataset is the same CXR dataset as used in our above\\nU-net CXR model. 528 CXRs are used for training and\\n176 for validation. The model was trained with a batch\\nsize of 12 and a learning rate of 10‚àí4 with the Adam\\noptimizer (Kingma & Ba, 2014) for a maximum of200\\nepoch using early stopping based on the validation loss.\\nThe Ô¨Årst run was90 epochs, and the second run was97.\\nC. Additional Experiments\\nC.1. Performance Impact of Carbontracker\\nThe performance impact of usingcarbontracker to moni-\\ntor all training epochs is shown as a boxplot in Figure 7.\\nWe see that the mean increase in epoch duration for our\\nU-net models across two runs is0.19% on DRIVE,1.06%\\non LIDC, and‚àí0.58% on CXR. While we see individual\\nepochs with a relative increase of up to5% on LIDC and\\neven 22% on DRIVE, this is more likely attributed to\\nthe stochasticity in epoch duration than tocarbontracker.\\nWe further note that the Ô¨Çuctuations in epoch dura-\\n11https://github.com/stefanknegt/\\nProbabilistic-Unet-Pytorch'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='Carbontracker\\nFigure 7.Box plot of the performance impact (%) of using\\ncarbontracker to monitor all training epochs shown as the\\nrelative increase in epoch duration compared to a baseline\\nwithout carbontracker. The whiskers and outliers are obtained\\nfrom the Tukey method using 1.5 times IQR.\\ntion (Figure 7) are not caused bycarbontracker. These\\nÔ¨Çuctuations are also witnessed in the baseline runs.\\nD. Estimating the\\nEnergy and Carbon Footprint of GPT-3\\nBrown et al. (2020) report that the GPT-3 model with\\n175 billion parameters used 3.14 ¬∑1023 Ô¨Çoating point\\noperations (FPOs) of compute to train using NVIDIA\\nV100 GPUs on a cluster provided by Microsoft. We\\nassume that these are the most powerful V100 GPUs,\\nthe V100S PCIe model, with a tensor performance of\\n130 TFLOPS12 and that the Microsoft data center has\\na PUE of 1.125, the average for new Microsoft data\\ncenters in 201513. The compute time on a single GPU is\\ntherefore\\n3.14¬∑1023 FPOs\\n130¬∑1012 FLOPS =2415384615.38s=27955 .84d.\\nThis is equivalent to about310 GPUs running non-stop\\nfor 90 days. If we use the thermal design power (TDP)\\nof the V100s and the PUE, we can estimate that this\\nused\\n250W¬∑2415384615.38s¬∑1.125=679326923075 .63J\\n=188701.92kWh.\\n12https://bit.ly/2zFsOK2\\n13http://download.microsoft.com/download/\\n8/2/9/8297f7c7-ae81-4e99-b1db-d65a01f7a8ef/\\nmicrosoft_cloud_infrastructure_datacenter_\\nand_network_fact_sheet.pdf\\nCarbon intensity\\n(gCO2eq/kWh)\\n13\\n819\\nFigure 8.Average carbon intensity ( gCO2eq/kWh) of\\nEU-28 countries in 2016. The intensity is calculated as\\nthe ratio of emissions from public electricity produc-\\ntion and gross electricity production. Data is provided\\nby the European Environment Agency (EEA). See\\nhttps://www.eea.europa.eu/ds_resolveuid/\\n3f6dc9e9e92b45b9b829152c4e0e7ade.\\nUsing the average carbon intensity of USA in 2017 of\\n449.06 gCO2eq/kWh14, we see this may emit up to\\n449.06gCO2eq/kWh¬∑188701.92kWh\\n=84738484.20gCO2eq\\n=84738.48kgCO2eq.\\nThis is equivalent to\\n84738484.20gCO2eq\\n120.4gCO2eqkm‚àí1 =703808.01km\\ntravelled by car using the averageCO2eq emissions of\\na newly registered car in the European Union in 201815.\\n14https://www.eia.gov/tools/faqs/faq.php?\\nid=74&t=11\\n15https://www.eea.europa.\\neu/data-and-maps/indicators/\\naverage-co2-emissions-from-motor-vehicles/\\nassessment-1'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='Carbontracker\\nFigure 9.Comparison of predicted and measured values of energy (kWh) and duration (s) per epoch when predicting after a\\nsingle epoch. (row 1) Energy. (row 2) Cumulative energy. (row 3) Duration. (row 4) Cumulative duration. Each column shows\\na different model and dataset, as detailed in Appendix B. The initial epoch is often characterized by low energy usage and\\nshort epoch duration compared to the following epochs. Notwithstanding, the linear prediction model used bycarbontracker\\nmay still lead to reasonable predictions after monitoring a single epoch.')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tiktoken import get_encoding\n",
        "\n",
        "def estimate_cost_from_documents(docs, cost_per_1k=0.0001):\n",
        "    \"\"\"\n",
        "    Given a list of Document objects (each with a 'page_content' attribute),\n",
        "    combine their text, count tokens using cl100k_base, and return the token count and cost.\n",
        "    \"\"\"\n",
        "    # Use the tokenizer (for GPT-3.5, GPT-4, text-embedding-ada-002)\n",
        "    encoder = get_encoding(\"cl100k_base\")\n",
        "\n",
        "    # Combine text from all documents into a single string\n",
        "    combined_text = \" \".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Encode the text to count tokens\n",
        "    tokens = encoder.encode(combined_text)\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    # Calculate estimated cost (cost per 1K tokens)\n",
        "    estimated_cost = (num_tokens / 1000) * cost_per_1k\n",
        "    return num_tokens, estimated_cost\n",
        "\n",
        "# Example usage:\n",
        "# Assume 'load_scientific_paper' is your function that returns the list of documents\n",
        "documents = load_scientific_paper(config.DOCUMENT_PATH)\n",
        "num_tokens, cost = estimate_cost_from_documents(documents, cost_per_1k=0.0001)\n",
        "print(f\"Number of tokens: {num_tokens}\")\n",
        "print(f\"Estimated cost: ${cost:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPD926mYeIJB",
        "outputId": "69e64fa9-b89d-4929-cacd-3a28d1b304bf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 11 documents.\n",
            "Preview of first document:\n",
            "Carbontracker: Tracking and Predicting the Carbon Footprint of Training\n",
            "Deep Learning Models\n",
            "Lasse F. Wolff Anthony‚àó1 Benjamin Kanding‚àó1 Raghavendra Selvan1\n",
            "Abstract\n",
            "Deep learning (DL) can achieve impressive\n",
            "results across a wide variety of tasks, but\n",
            "this often comes at the cost of training\n",
            "models for extensive periods on specialized\n",
            "hardware accelerators. This energy-intensive\n",
            "workload has seen immense growth in recent\n",
            "years. Machine learning (ML) may become\n",
            "a signiÔ¨Åcant contributor to climate\n",
            "Number of tokens: 10430\n",
            "Estimated cost: $0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_scientific_text(documents):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        chunk_size=config.CHUNK_SIZE,\n",
        "        chunk_overlap=config.CHUNK_OVERLAP,\n",
        "        separator=\"\\n\"\n",
        "    )\n",
        "    split_docs = text_splitter.split_documents(documents)\n",
        "    # Debugging: print how many chunks were created\n",
        "    print(f\"Document split into {len(split_docs)} chunks.\")\n",
        "    return split_docs\n",
        "\n",
        "split_scientific_text(load_scientific_paper(config.DOCUMENT_PATH))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwehiftnZW8r",
        "outputId": "f351028c-5a6d-403f-986c-3e08af892ef5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 11 documents.\n",
            "Preview of first document:\n",
            "Carbontracker: Tracking and Predicting the Carbon Footprint of Training\n",
            "Deep Learning Models\n",
            "Lasse F. Wolff Anthony‚àó1 Benjamin Kanding‚àó1 Raghavendra Selvan1\n",
            "Abstract\n",
            "Deep learning (DL) can achieve impressive\n",
            "results across a wide variety of tasks, but\n",
            "this often comes at the cost of training\n",
            "models for extensive periods on specialized\n",
            "hardware accelerators. This energy-intensive\n",
            "workload has seen immense growth in recent\n",
            "years. Machine learning (ML) may become\n",
            "a signiÔ¨Åcant contributor to climate\n",
            "Document split into 36 chunks.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Carbontracker: Tracking and Predicting the Carbon Footprint of Training\\nDeep Learning Models\\nLasse F. Wolff Anthony‚àó1 Benjamin Kanding‚àó1 Raghavendra Selvan1\\nAbstract\\nDeep learning (DL) can achieve impressive\\nresults across a wide variety of tasks, but\\nthis often comes at the cost of training\\nmodels for extensive periods on specialized\\nhardware accelerators. This energy-intensive\\nworkload has seen immense growth in recent\\nyears. Machine learning (ML) may become\\na signiÔ¨Åcant contributor to climate change\\nif this exponential trend continues. If practi-\\ntioners are aware of their energy and carbon\\nfootprint, then they may actively take steps to\\nreduce it whenever possible. In this work, we\\npresent carbontracker, a tool for tracking and\\npredicting the energy and carbon footprint of\\ntraining DL models. We propose that energy\\nand carbon footprint of model development\\nand training is reported alongside perfor-\\nmance metrics using tools like carbontracker.\\nWe hope this will promote responsible\\ncomputing in ML and encourage research\\ninto energy-efÔ¨Åcient deep neural networks.1\\n1. Introduction\\nThe popularity of solving problems using deep learn-\\ning (DL) has rapidly increased and with it the need\\nfor ever more powerful models. These models achieve\\nimpressive results across a wide variety of tasks such as\\ngameplay, where AlphaStar reached the highest rank in\\nthe strategy game Starcraft II (Vinyals et al., 2019) and\\nAgent57 surpassed human performance in all 57 Atari'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='for ever more powerful models. These models achieve\\nimpressive results across a wide variety of tasks such as\\ngameplay, where AlphaStar reached the highest rank in\\nthe strategy game Starcraft II (Vinyals et al., 2019) and\\nAgent57 surpassed human performance in all 57 Atari\\n2600 games (Badia et al., 2020). This comes at the cost\\nof training the model for thousands of hours on special-\\n*Equal contribution 1Department of Computer Sci-\\nence, University of Copenhagen, Copenhagen, Denmark.\\nCorrespondence to: Lasse F. Wolff Anthony <lassewolffan-\\nthony@gmail.com>, Benjamin Kanding <bmk1212@live.dk>.\\nICML Workshop on \"Challenges in Deploying and monitor-\\ning Machine Learning Systems\", 2020.\\n1Source code for carbontracker is available here:\\nhttps://github.com/lfwa/carbontracker\\nized hardware accelerators such as graphics processing\\nunits (GPUs). From 2012 to 2018 the compute needed\\nfor DL grew300000-fold (Amodei & Hernandez, 2018).\\nThis immense growth in required compute has a\\nhigh energy demand, which in turn increases the\\ndemand for energy production. In 2010 energy\\nproduction was responsible for approximately35% of\\ntotal anthropogenic greenhouse gas (GHG) emissions\\n(Bruckner et al., 2014). Should this exponential trend in\\nDL compute continue then machine learning (ML) may\\nbecome a signiÔ¨Åcant contributor to climate change.\\nThis can be mitigated by exploring how to improve\\nenergy efÔ¨Åciency in DL. Moreover, if practitioners are\\naware of their energy and carbon footprint, then they'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='DL compute continue then machine learning (ML) may\\nbecome a signiÔ¨Åcant contributor to climate change.\\nThis can be mitigated by exploring how to improve\\nenergy efÔ¨Åciency in DL. Moreover, if practitioners are\\naware of their energy and carbon footprint, then they\\nmay actively take steps to reduce it whenever possible.\\nWe show that in ML, these can be simple steps that\\nresult in considerable reductions to carbon emissions.\\nThe environmental impact of ML in research and\\nindustry has seen increasing interest in the last year\\nfollowing the 2018 IPCC special report (IPCC, 2018)\\ncalling for urgent action in order to limit global\\nwarming to 1.5 ‚ó¶C. We brieÔ¨Çy review some notable\\nwork on the topic. Strubell et al. (2019) estimated the\\nÔ¨Ånancial and environmental costs of R&D and hyper-\\nparameter tuning for various state-of-the-art (SOTA)\\nneural network (NN) models in natural language\\nprocessing (NLP). They point out that increasing cost\\nand emissions of SOTA models contribute to a lack\\nof equity between those researchers who have access\\nto large-scale compute, and those who do not. The\\nauthors recommend that metrics such as training time,\\ncomputational resources required, and model sensi-\\ntivity to hyperparameters should be reported to enable\\ndirect comparison between models. Lacoste et al. (2019)\\nprovided the Machine Learning Emissions Calculatorthat\\nrelies on self-reporting. The tool can estimate the car-\\nbon footprint of GPU compute by specifying hardware'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='tivity to hyperparameters should be reported to enable\\ndirect comparison between models. Lacoste et al. (2019)\\nprovided the Machine Learning Emissions Calculatorthat\\nrelies on self-reporting. The tool can estimate the car-\\nbon footprint of GPU compute by specifying hardware\\ntype, hours used, cloud provider, and region. Hender-\\nson et al. (2020) presented theexperiment-impact-tracker\\nframework and gave various strategies for mitigating\\ncarbon emissions in ML. Their Python framework\\nallows for estimating the energy and carbon impact\\narXiv:2007.03051v1  [cs.CY]  6 Jul 2020'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='Carbontracker\\nof ML systems as well as the generation of ‚ÄúCarbon\\nImpact Statements‚Äù for standardized reporting hereof.\\nIn this work, we propose carbontracker, a tool for\\ntracking and predicting the energy consumption\\nand carbon emissions of training DL models. The\\nmethodology is similar to that of Henderson et al.\\n(2020) but differs from prior art in two major ways:\\n(1) We allow for a further proactive and intervention-\\ndriven approach to reducing carbon emissions\\nby supporting predictions. Model training can be\\nstopped, at the user‚Äôs discretion, if the predicted\\nenvironmental cost is exceeded.\\n(2) We support a variety of different environments\\nand platforms such as clusters, desktop comput-\\ners, and Google Colab notebooks, allowing for a\\nplug-and-play experience.\\nWe experimentally evaluate the tool on several different\\ndeep convolutional neural network (CNN) architec-\\ntures and datasets for medical image segmentation\\nand assess the accuracy of its predictions. We present\\nconcrete recommendations on how to reduce carbon\\nemissions considerably when training DL models.\\n2. Design and Implementation\\nThe design philosophy that guided the development\\nof carbontracker can be summarized by the following\\nprinciples:\\nPythonic The majority of ML takes place in the Python\\nlanguage (Wilcox et al., 2017). We want the tool to\\nbe as easy as possible to integrate into existing work\\nenvironments making Python the language of choice.\\nUsable The required effort and added code must be'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='principles:\\nPythonic The majority of ML takes place in the Python\\nlanguage (Wilcox et al., 2017). We want the tool to\\nbe as easy as possible to integrate into existing work\\nenvironments making Python the language of choice.\\nUsable The required effort and added code must be\\nminimal and not obfuscate the existing code structure.\\nExtensible Adding and maintaining support for\\nchanging application programming interfaces (APIs)\\nand new hardware should be straightforward.\\nFlexible The user should have full control over what\\nis monitored and how this monitoring is performed.\\nPerformance The performance impact of using the\\ntool must be negligible, and computation should be\\nminimal. It must not affect training.\\nInterpretable Carbon footprint expressed ingCO2eq\\nis often meaningless. A common understanding of the\\nimpact should be facilitated through conversions.\\nCarbontracker is an open-source tool written in Python\\nfor tracking and predicting the energy consumption\\nand carbon emissions of training DL models. It is avail-\\nable through the Python Package Index (PyPi). The tool\\nis implemented as a multithreaded program. It utilizes\\nseparate threads to collect power measurements and\\nfetch carbon intensity in real-time for parallel efÔ¨Åciency\\nand to not disrupt the model training in the main thread.\\nAppendix A has further implementation details.\\nCarbontracker supports predicting the total duration,\\nenergy, and carbon footprint of training a DL model.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='fetch carbon intensity in real-time for parallel efÔ¨Åciency\\nand to not disrupt the model training in the main thread.\\nAppendix A has further implementation details.\\nCarbontracker supports predicting the total duration,\\nenergy, and carbon footprint of training a DL model.\\nThese predictions are based on a user-speciÔ¨Åed number\\nof monitored epochs with a default of 1. We forecast\\nthe carbon intensity of electricity production during\\nthe predicted duration using the supported APIs. The\\nforecasted carbon intensity is then used to predict the\\ncarbon footprint. Following our preliminary research,\\nwe use a simple linear model for predictions.\\n3. Experiments and Results\\nIn order to evaluate the performance and behavior\\nof carbontracker, we conducted experiments on three\\nmedical image datasets using two different CNN\\nmodels: U-net (Ronneberger et al., 2015) and lungVAE\\n(Selvan et al., 2020). The models were trained for\\nthe task of medical image segmentation using three\\ndatasets: DRIVE (Staal et al., 2004), LIDC (Armato III\\net al., 2004), and CXR (Jaeger et al., 2014). Details on\\nthe models and datasets are given in Appendix B. All\\nmeasurements were taken usingcarbontracker version\\n1.1.2. We performed our experiments on a single\\nNVIDIA TITAN RTX GPU with 12 GB memory and\\ntwo Intel central processing units (CPUs).\\nIn line with our message of reporting energy and\\ncarbon footprint, we used carbontracker to generate\\nthe following statement: The training of models in'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='1.1.2. We performed our experiments on a single\\nNVIDIA TITAN RTX GPU with 12 GB memory and\\ntwo Intel central processing units (CPUs).\\nIn line with our message of reporting energy and\\ncarbon footprint, we used carbontracker to generate\\nthe following statement: The training of models in\\nthis work is estimated to use37.445 kWhof electricity\\ncontributing to 3.166 kgof CO2eq. This is equivalent\\nto 26.296 kmtravelled by car (see A.5).\\nAn overview of predictions fromcarbontracker based on\\nmonitoring for 1 training epoch for the trained models\\ncompared to the measured values is shown in Figure 1.\\nThe errors in the energy predictions are4.9‚Äì19.1% com-\\npared to the measured energy values,7.3‚Äì19.9% for the\\nCO2eq, and 0.8‚Äì4.6% for the duration. The error in the\\nCO2eq predictions are also affected by the quality of the\\nforecasted carbon intensity from the APIs used bycar-\\nbontracker. This is highlighted in Figure 2, which shows\\nthe estimated carbon emissions (gCO2eq) of training\\nour U-net model on LIDC in Denmark and Great\\nBritain for different carbon intensity estimation meth-\\nods. As also shown by Henderson et al. (2020), we see\\nthat using country or region-wide average estimates'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='Carbontracker\\nFigure 1.Comparison of predicted and measured values of\\nenergy in kWh (left), emissions ingCO2eq (center), and du-\\nration in s (right) for the full training session when predicting\\nafter a single epoch. The diagonal line represents predic-\\ntions that are equal to the actual measured consumption.\\nDescription of the models and datasets are in Appendix B.\\nFigure 2.Carbon emissions (gCO2eq) of training the U-net on\\nLIDC dataset for different carbon intensity estimation meth-\\nods. (left) The emissions of training in Denmark and (right)\\nin Great Britain at 2020-05-21 22:00 local time. Real-time indi-\\ncates that the current intensity is fetched every15 minduring\\ntraining using the APIs supported by carbontracker. The\\naverage intensities are from 2016 (see Figure 8 in Appendix).\\nmay severely overestimate (or under different circum-\\nstances underestimate) emissions. This illustrates the\\nimportance of using real-time (or forecasted) carbon\\nintensity for accurate estimates of carbon footprint.\\nFigure 3 summarizes the relative energy consumption\\nof each component across all runs. We see that\\nwhile the GPU uses the majority of the total energy,\\naround 50‚Äì60%, the CPU and dynamic random-access\\nmemory (DRAM) also account for a signiÔ¨Åcant part\\nof the total consumption. This is consistent with the\\nÔ¨Åndings of Gorkovenko & Dholakia (2020), who found\\nthat GPUs are responsible for around 70% of power\\nconsumption, CPU for 15%, and RAM for 10% when'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='around 50‚Äì60%, the CPU and dynamic random-access\\nmemory (DRAM) also account for a signiÔ¨Åcant part\\nof the total consumption. This is consistent with the\\nÔ¨Åndings of Gorkovenko & Dholakia (2020), who found\\nthat GPUs are responsible for around 70% of power\\nconsumption, CPU for 15%, and RAM for 10% when\\ntesting on the TensorFlow benchmarking suite for DL\\non Lenovo ThinkSystem SR670 servers. As such, only\\naccounting for GPU consumption when quantifying\\nthe energy and carbon footprint of DL models will lead\\nto considerable underestimation of the actual footprint.\\n4. Reducing Your Carbon Footprint\\nThe carbon emissions that occur when training DL\\nmodels are not irreducible and do not have to simply\\nbe the cost of progress within DL. Several steps can\\nFigure 3.Comparison of energy usage by component shown\\nas the relative energy usage (%) out of the total energy spent\\nduring training. We see that the GPU uses the majority of the\\nenergy, about50‚Äì60%, but the CPU and DRAM also account\\nfor a signiÔ¨Åcant amount of the total energy consumption\\nacross all models and datasets.\\nFigure 4.Estimated carbon emissions (gCO2eq) of training\\nour models (see Appendix B) in different EU-28 countries.\\nThe calculations are based on the average carbon intensities\\nfrom 2016 (see Figure 8 in Appendix).\\nbe taken in order to reduce this footprint considerably.\\nIn this section, we outline some strategies for practi-\\ntioners to directly mitigate their carbon footprint when\\ntraining DL models.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='The calculations are based on the average carbon intensities\\nfrom 2016 (see Figure 8 in Appendix).\\nbe taken in order to reduce this footprint considerably.\\nIn this section, we outline some strategies for practi-\\ntioners to directly mitigate their carbon footprint when\\ntraining DL models.\\nLow Carbon Intensity Regions The carbon intensity\\nof electricity production varies by region and is\\ndependent on the energy sources that power the local\\nelectrical grid. Figure 4 illustrates how the variation\\nin carbon intensity between regions can inÔ¨Çuence the\\ncarbon footprint of training DL models. Based on the\\n2016 average intensities, we see that a model trained\\nin Estonia may emit more than 61 times theCO2eq as\\nan equivalent model would when trained in Sweden.\\nIn perspective, our U-net model trained on the LIDC\\ndataset would emit 17.7 gCO2eq or equivalently the\\nsame as traveling 0.14 kmby car when trained in\\nSweden. However, training in Estonia it would emit\\n1087.9 gCO2eq or the same as traveling9.04 kmby car\\nfor just a single training session.\\nAs training DL models is generally not latency bound,\\nwe recommend that ML practitioners move training\\nto regions with a low carbon intensity whenever it is\\npossible to do so. We must further emphasize that for'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='Carbontracker\\nFigure 5.Real-time carbon intensity ( gCO2eq/kWh) for\\nDenmark (DK) and Great Britain (GB) from 2020-05-18 to\\n2020-05-25 shown in local time. The data is collected using\\nthe APIs supported bycarbontracker. The carbon intensities\\nare volatile to changes in energy demand and depend on the\\nenergy sources available.\\nlarge-scale models that are trained on multiple GPUs\\nfor long periods, such as OpenAI‚Äôs GPT-3 language\\nmodel (Brown et al., 2020), it is imperative that training\\ntakes place in low carbon intensity regions in order\\nto avoid several megagrams of carbon emissions.\\nThe absolute difference in emissions may even be\\nsigniÔ¨Åcant between two green regions, like Sweden\\nand France, for such large-scale runs.\\nTraining Times The time period in which a DL model\\nis trained affects its overall carbon footprint. This\\nis caused by carbon intensity changing throughout\\nthe day as energy demand and capacity of energy\\nsources change. Figure 5 shows the carbon intensity\\n(gCO2eq/kWh) for Denmark and Great Britain in the\\nweek of 2020-05-18 to 2020-05-25 collected with the\\nAPIs supported bycarbontracker. A model trained dur-\\ning low carbon intensity hours of the day in Denmark\\nmay emit as little as 1\\n4 the CO2eq of one trained during\\npeak hours. A similar trend can be seen for Great\\nBritain, where 2-fold savings in emissions can be had.\\nWe suggest that ML practitioners shift training to take\\nplace in low carbon intensity time periods whenever'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='may emit as little as 1\\n4 the CO2eq of one trained during\\npeak hours. A similar trend can be seen for Great\\nBritain, where 2-fold savings in emissions can be had.\\nWe suggest that ML practitioners shift training to take\\nplace in low carbon intensity time periods whenever\\npossible. The time period should be determined on a\\nregional level.\\nEfÔ¨Åcient Algorithms The use of efÔ¨Åcient algorithms\\nwhen training DL models can further help reduce\\ncompute-resources and thereby also carbon emissions.\\nHyperparameter tuning may be improved by substitut-\\ning grid search for random search (Bergstra & Bengio,\\n2012), using Bayesian optimization (Snoek et al., 2012)\\nor other optimization techniques like Hyperband (Li\\net al., 2017). Energy efÔ¨Åciency of inference in deep\\nneural networks (DNNs) is also an active area of\\nresearch with methods such as quantization aware\\ntraining, energy-aware pruning (Yang et al., 2017),\\nand power- and memory-constrained hyperparameter\\noptimization like HyperPower (Stamoulis et al., 2018).\\nEfÔ¨Åcient Hardware and Settings Choosing more\\nenergy-efÔ¨Åcient computing hardware and settings may\\nalso contribute to reducing carbon emissions. Some\\nGPUs have substantially higher efÔ¨Åciency in terms\\nof Ô¨Çoating point operations per second (FLOPS) per\\nwatt of power usage compared to others (Lacoste et al.,\\n2019). Power management techniques like dynamic\\nvoltage and frequency scaling (DVFS) can further help\\nconserve energy consumption (Li et al., 2016) and for'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='of Ô¨Çoating point operations per second (FLOPS) per\\nwatt of power usage compared to others (Lacoste et al.,\\n2019). Power management techniques like dynamic\\nvoltage and frequency scaling (DVFS) can further help\\nconserve energy consumption (Li et al., 2016) and for\\nsome models even reduce time to reach convergence\\n(Tang et al., 2019). Tang et al. (2019) show that DVFS\\ncan be applied to GPUs to help conserve about8.7% to\\n23.1% energy consumption for training different DNNs\\nand about 19.6% to 26.4% for inference. Moreover, the\\nauthors show that the default frequency settings on\\ntested GPUs, such as NVIDIA‚Äôs Pascal P100 and Volta\\nV100, are often not optimized for energy efÔ¨Åciency in\\nDNN training and inference.\\n5. Discussion and Conclusion\\nThe current trend in DL is a rapidly increasing demand\\nfor compute that does not appear to slow down.\\nThis is evident in recent models such as the GPT-3\\nlanguage model (Brown et al., 2020) with 175 billion\\nparameters requiring an estimated28000 GPU-days to\\ntrain excluding R&D (see Appendix D). We hope to\\nspread awareness about the environmental impact of\\nthis increasing compute through accurate reporting\\nwith the use of tools such as carbontracker. Once\\ninformed, concrete and often simple steps can be taken\\nin order to reduce the impact.\\nSOTA-results in DL are frequently determined by a\\nmodel‚Äôs performance through metrics such as accuracy,\\nAUC score, or similar performance metrics. Energy-\\nefÔ¨Åciency is usually not one of these. While such'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='informed, concrete and often simple steps can be taken\\nin order to reduce the impact.\\nSOTA-results in DL are frequently determined by a\\nmodel‚Äôs performance through metrics such as accuracy,\\nAUC score, or similar performance metrics. Energy-\\nefÔ¨Åciency is usually not one of these. While such\\nperformance metrics remain a crucial measure of model\\nsuccess, we hope to promote an increasing focus on\\nenergy-efÔ¨Åciency. We must emphasize that we do not\\nargue that compute-intensive research is not essential\\nfor the progress of DL. We believe, however, that the im-\\npact of this compute should be minimized. We propose\\nthat the total energy and carbon footprint of model de-\\nvelopment and training is reported alongside accuracy\\nand similar metrics to promote responsible computing\\nin ML and research into energy-efÔ¨Åcient DNNs.\\nIn this work, we showed that ML risks becoming a\\nsigniÔ¨Åcant contributor to climate change. To this end,\\nwe introduced the open-source carbontracker tool for\\ntracking and predicting the total energy consumption\\nand carbon emissions of training DL models. This\\nenables practitioners to be aware of their footprint and\\ntake action to reduce it.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='Carbontracker\\nACKNOWLEDGEMENTS\\nThe authors would like to thank Morten Pol Engell-\\nN√∏rreg√•rd for the thorough feedback on the thesis\\nversion of this work. The authors also thank the\\nanonymous reviewers and early users ofcarbontracker\\nfor their insightful feedback.\\nReferences\\nAmodei, D. and Hernandez, D. Ai and compute.Herun-\\ntergeladen von https://blog. openai. com/aiand-compute,\\n2018.\\nArmato III, S. G., McLennan, G., McNitt-Gray,\\nM. F., Meyer, C. R., Yankelevitz, D., Aberle, D. R.,\\nHenschke, C. I., Hoffman, E. A., Kazerooni, E. A.,\\nMacMahon, H., et al. Lung image database consor-\\ntium: developing a resource for the medical imaging\\nresearch community.Radiology, 232(3):739‚Äì748, 2004.\\nAscierto, R. Uptime Institute 2018 Data Center Survey.\\nTechnical report, Uptime Institute, 2018.\\nAscierto, R. Uptime Institute 2019 Data Center Survey.\\nTechnical report, Uptime Institute, 2019.\\nAvelar, V ., Azevedo, D., and French, A. PUE‚Ñ¢ : A Com-\\nprehensive Examination of the Metric. GreenGrid,\\npp. 1‚Äì83, 2012.\\nBadia, A. P ., Piot, B., Kapturowski, S., Sprechmann,\\nP ., Vitvitskyi, A., Guo, D., and Blundell, C. Agent57:\\nOutperforming the atari human benchmark, 2020.\\nBergstra, J. and Bengio, Y. Random search for hyper-\\nparameter optimization. Journal of Machine Learning\\nResearch, 13:281‚Äì305, 2012. ISSN 15324435.\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P ., Neelakantan, A., Shyam, P ., Sastry,\\nG., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='parameter optimization. Journal of Machine Learning\\nResearch, 13:281‚Äì305, 2012. ISSN 15324435.\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P ., Neelakantan, A., Shyam, P ., Sastry,\\nG., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,\\nG., Henighan, T., Child, R., Ramesh, A., Ziegler,\\nD. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler,\\nE., Litwin, M., Gray, S., Chess, B., Clark, J., Berner,\\nC., McCandlish, S., Radford, A., Sutskever, I., and\\nAmodei, D. Language models are few-shot learners,\\n2020.\\nBruckner, T., Bashmakov, Y., Mulugetta, H., and\\nChum, A. 2014: Energy systems. In Climate Change\\n2014: Mitigation of Climate Change. Contribution of\\nWorking Group III to the Fifth Assessment Report of the\\nIntergovernmental Panel on Climate Change. 2014.\\nDavid, H., Gorbatov, E., Hanebutte, U. R., Khanna, R.,\\nand Le, C. RAPL: Memory power estimation and\\ncapping. In Proceedings of the International Symposium\\non Low Power Electronics and Design, pp. 189‚Äì194, 2010.\\nISBN 9781450301466. doi: 10.1145/1840845.1840883.\\nGoodward, J. and Kelly, A. Bottom line on offsets.\\nTechnical report, The World Resources Institute, 10\\nG Street, NE Suite 800 Washington, D. C . . . , 2010.\\nGorkovenko, M. and Dholakia, A. Towards Power Ef-\\nÔ¨Åciency in Deep Learning on Data Center Hardware.\\npp. 1814‚Äì1820, 2020.\\nHenderson, P ., Hu, J., Romoff, J., Brunskill, E.,\\nJurafsky, D., and Pineau, J. Towards the Sys-\\ntematic Reporting of the Energy and Carbon'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='Gorkovenko, M. and Dholakia, A. Towards Power Ef-\\nÔ¨Åciency in Deep Learning on Data Center Hardware.\\npp. 1814‚Äì1820, 2020.\\nHenderson, P ., Hu, J., Romoff, J., Brunskill, E.,\\nJurafsky, D., and Pineau, J. Towards the Sys-\\ntematic Reporting of the Energy and Carbon\\nFootprints of Machine Learning. jan 2020. URL\\nhttp://arxiv.org/abs/2002.05651.\\nIPCC. Global Warming of 1.5 ¬∞C. An IPCC Special\\nReport on the impacts of global warming of 1.5 ¬∞C\\nabove pre-industrial levels and related global\\ngreenhouse gas emission pathways, in the context\\nof strengthening the global response to the threat of\\nclimate change,. Technical report, 2018.\\nJaeger, S., Candemir, S., Antani, S., W√°ng, Y.-X. J.,\\nLu, P .-X., and Thoma, G. Two public chest x-ray\\ndatasets for computer-aided screening of pulmonary\\ndiseases. Quantitative imaging in medicine and surgery,\\n4(6):475, 2014.\\nKingma, D. and Ba, J. Adam optimizer.arXiv preprint\\narXiv:1412.6980, pp. 1‚Äì15, 2014.\\nLacoste, A., Luccioni, A., Schmidt, V ., and Dandres,\\nT. Quantifying the Carbon Emissions of Machine\\nLearning. Technical report, 2019.\\nLi, D., Chen, X., Becchi, M., and Zong, Z. Evaluating\\nthe energy efÔ¨Åciency of deep convolutional neural\\nnetworks on CPUs and GPUs. InProceedings - 2016\\nIEEE International Conferences on Big Data and Cloud\\nComputing, BDCloud 2016, Social Computing and Net-\\nworking, SocialCom 2016 and Sustainable Computing\\nand Communications, SustainCom 2016, pp. 477‚Äì484.\\nInstitute of Electrical and Electronics Engineers'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='IEEE International Conferences on Big Data and Cloud\\nComputing, BDCloud 2016, Social Computing and Net-\\nworking, SocialCom 2016 and Sustainable Computing\\nand Communications, SustainCom 2016, pp. 477‚Äì484.\\nInstitute of Electrical and Electronics Engineers\\nInc., oct 2016. ISBN 9781509039364. doi: 10.1109/\\nBDCloud-SocialCom-SustainCom.2016.76.\\nLi, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A.,\\nand Talwalkar, A. Hyperband: A novel bandit-based\\napproach to hyperparameter optimization. J. Mach.\\nLearn. Res., 18(1):6765‚Äì6816, January 2017. ISSN\\n1532-4435.\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury,\\nJ., Chanan, G., Killeen, T., Lin, Z., Gimelshein,\\nN., Antiga, L., Desmaison, A., K√∂pf, A., Yang,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='Carbontracker\\nE., DeVito, Z., Raison, M., Tejani, A., Chil-\\namkurthy, S., Steiner, B., Fang, L., Bai, J., and\\nChintala, S. PyTorch: An Imperative Style, High-\\nPerformance Deep Learning Library. 2019. URL\\nhttp://arxiv.org/abs/1912.01703.\\nRonneberger, O., Fischer, P ., and Brox, T. U-net: Con-\\nvolutional networks for biomedical image segmen-\\ntation. In Lecture Notes in Computer Science (including\\nsubseries Lecture Notes in ArtiÔ¨Åcial Intelligence and\\nLecture Notes in Bioinformatics), volume 9351, pp. 234‚Äì\\n241, may 2015. doi: 10.1007/978-3-319-24574-4_28.\\nURL http://arxiv.org/abs/1505.04597.\\nSelvan, R., Dam, E. B., Detlefsen, N. S., Rischel,\\nS., Sheng, K., Nielsen, M., and Pai, A. Lung\\nSegmentation from Chest X-rays using Variational\\nData Imputation. In ICML Workshop on The Art of\\nLearning with Missing Values , 2020. URL https:\\n//openreview.net/forum?id=dlzQM28tq2W.\\nSnoek, J., Larochelle, H., and Adams, R. P . Practical\\nbayesian optimization of machine learning algo-\\nrithms. In Advances in neural information processing\\nsystems, pp. 2951‚Äì2959, 2012.\\nStaal, J., Abr√†moff, M. D., Niemeijer, M., Viergever,\\nM. A., and Van Ginneken, B. Ridge-based vessel\\nsegmentation in color images of the retina. IEEE\\nTransactions on Medical Imaging, 23(4):501‚Äì509, apr\\n2004. ISSN 02780062. doi: 10.1109/TMI.2004.825627.\\nStamoulis, D., Cai, E., Juan, D. C., and Marculescu,\\nD. HyperPower: Power- and memory-constrained\\nhyper-parameter optimization for neural networks.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='segmentation in color images of the retina. IEEE\\nTransactions on Medical Imaging, 23(4):501‚Äì509, apr\\n2004. ISSN 02780062. doi: 10.1109/TMI.2004.825627.\\nStamoulis, D., Cai, E., Juan, D. C., and Marculescu,\\nD. HyperPower: Power- and memory-constrained\\nhyper-parameter optimization for neural networks.\\nIn Proceedings of the 2018 Design, Automation and\\nTest in Europe Conference and Exhibition, DATE 2018,\\nvolume 2018-Janua, pp. 19‚Äì24, dec 2018. ISBN\\n9783981926316. doi: 10.23919/DATE.2018.8341973.\\nURL http://arxiv.org/abs/1712.02446.\\nStrubell, E., Ganesh, A., and McCallum, A. Energy and\\nPolicy Considerations for Deep Learning in NLP. pp.\\n3645‚Äì3650, 2019. doi: 10.18653/v1/p19-1355. URL\\nhttps://bit.ly/2JTbGnI.\\nTang, Z., Wang, Y., Wang, Q., and Chu, X. The impact\\nof GPU DVFS on the energy and performance of\\ndeep Learning: An Empirical Study. Ine-Energy 2019\\n- Proceedings of the 10th ACM International Conference\\non Future Energy Systems , pp. 315‚Äì325, may 2019.\\nISBN 9781450366717. doi: 10.1145/3307772.3328315.\\nURL http://arxiv.org/abs/1905.11012.\\nVinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu,\\nM., Dudzik, A., Chung, J., Choi, D. H., Powell, R.,\\nEwalds, T., Georgiev, P ., et al. Grandmaster level in\\nstarcraft ii using multi-agent reinforcement learning.\\nNature, 575(7782):350‚Äì354, 2019.\\nWilcox, M., Schuermans, S., Voskoglou, C., and\\nSobolevski, A. Developer economics: State of the\\ndeveloper nation q1 2017. Technical report, 2017.\\nYang, T. J., Chen, Y. H., and Sze, V . Designing'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='starcraft ii using multi-agent reinforcement learning.\\nNature, 575(7782):350‚Äì354, 2019.\\nWilcox, M., Schuermans, S., Voskoglou, C., and\\nSobolevski, A. Developer economics: State of the\\ndeveloper nation q1 2017. Technical report, 2017.\\nYang, T. J., Chen, Y. H., and Sze, V . Designing\\nenergy-efÔ¨Åcient convolutional neural networks\\nusing energy-aware pruning. In Proceedings\\n- 30th IEEE Conference on Computer Vision\\nand Pattern Recognition, CVPR 2017 , volume\\n2017-Janua, pp. 6071‚Äì6079, nov 2017. ISBN\\n9781538604571. doi: 10.1109/CVPR.2017.643. URL\\nhttp://arxiv.org/abs/1611.05128.\\nYuventi, J. and Mehdizadeh, R. A critical analysis of\\nPower Usage Effectiveness and its use in commu-\\nnicating data center energy consumption. Energy\\nand Buildings, 64:90‚Äì94, 2013. ISSN 03787788. doi:\\n10.1016/j.enbuild.2013.04.015.\\nA. Implementation details\\nListing 1.Example of the default setup added to training\\nscripts for tracking and predicting withcarbontracker.\\n1 from carbontracker.tracker\\nimport CarbonTracker‚Ü™‚Üí\\n2\\n3\\n4 tracker\\n= CarbonTracker(epochs=<your epochs>)‚Ü™‚Üí\\n5\\n6 for epoch in range(<your epochs>):\\n7 tracker.epoch_start()\\n8\\n9 # Your model training.\\n10\\n11 tracker.epoch_end()\\n12\\n13 tracker.stop()\\nCarbontracker is a multithreaded program. Figure 6\\nillustrates a high-level overview of the program.\\nA.1. On\\nthe Topic of Power and Energy Measurements\\nIn our work, we measure the total power of selected\\ncomponents such as the GPU, CPU, and DRAM. It\\ncan be argued that dynamic power rather than total'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='Carbontracker is a multithreaded program. Figure 6\\nillustrates a high-level overview of the program.\\nA.1. On\\nthe Topic of Power and Energy Measurements\\nIn our work, we measure the total power of selected\\ncomponents such as the GPU, CPU, and DRAM. It\\ncan be argued that dynamic power rather than total\\npower would more fairly represent a user‚Äôs power\\nconsumption when using large clusters or cloud\\ncomputing. We argue that these computing resources\\nwould not have to exist if the user did not use them. As'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='Carbontracker\\nListing 2.Example output of usingcarbontracker to track and\\npredict the energy and carbon footprint of training a DL\\nmodel.\\nCarbonTracker: The following components\\nwere found: GPU with device(s) TITAN\\nRTX. CPU with device(s) cpu:0, cpu:1.\\n‚Ü™‚Üí\\n‚Ü™‚Üí\\nCarbonTracker: Carbon intensity\\nfor the next 1:54:54 is predicted to\\nbe 54.09 gCO2/kWh at detected location:\\nCopenhagen, Capital Region, DK.\\n‚Ü™‚Üí\\n‚Ü™‚Üí\\n‚Ü™‚Üí\\nCarbonTracker:\\nPredicted consumption for 100 epoch(s):\\nTime: 1:54:54\\nEnergy: 1.159974 kWh\\nCO2eq: 62.744032 g\\nThis is equivalent to:\\n0.521130 km travelled by car\\nCarbonTracker: Average\\ncarbon intensity during training\\nwas 58.25 gCO2/kWh at detected location:\\nCopenhagen, Capital Region, DK.\\n‚Ü™‚Üí\\n‚Ü™‚Üí\\n‚Ü™‚Üí\\nCarbonTracker:\\nActual consumption for 100 epoch(s):\\nTime: 1:55:55\\nEnergy: 1.334319 kWh\\nCO2eq: 77.724065 g\\nThis is equivalent to:\\n0.645549 km travelled by car\\nCarbonTracker: Finished monitoring.\\nsuch, the user should also be accountable for the static\\npower consumption during the period in which they\\nreserve the resource. It is also a pragmatic solution as\\naccurately estimating dynamic power is challenging\\ndue to the infeasibility of measuring static power by\\nsoftware and the difÔ¨Åculty in storing and updating\\ninformation about static power for a multitude of\\ndifferent components. A similar argument can be made\\nfor the inclusion of life-cycle aspects in our energy\\nestimates, such as accounting for the energy attributed\\nto the manufacturing of system components. Like'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='information about static power for a multitude of\\ndifferent components. A similar argument can be made\\nfor the inclusion of life-cycle aspects in our energy\\nestimates, such as accounting for the energy attributed\\nto the manufacturing of system components. Like\\nHenderson et al. (2020), we ignore these aspects due\\nto the difÔ¨Åculties in their estimation.\\nThe power and energy monitoring in carbontracker is\\nlimited to a few main components of computational\\nsystems. Additional power consumed by the support-\\ning infrastructure, such as that used for cooling or\\npower delivery, is accounted for by multiplying the\\nmeasured power by the Power Usage Effectiveness\\n(PUE) of the data center hosting the compute, as sug-\\ngested by Strubell et al. (2019). PUE is a ratio describing\\nthe efÔ¨Åciency of a data center and the energy overhead\\nof the computing equipment. It is deÔ¨Åned as the ratio\\nof the total energy used in a data center facility to the\\nenergy used by the IT equipment such as the compute,\\nstorage, and network equipment (Avelar et al., 2012):\\nPUE= Total Facility Energy\\nIT Equipment Energy. (1)\\nPrevious research has examined PUE and its shortcom-\\nings (Yuventi & Mehdizadeh, 2013). These shortcom-\\nings may largely be resolved by data centers reporting\\nan average PUE instead of a minimum observed value.\\nIn our work, we use a PUE of1.58, the global average\\nfor data centers in 2018 as reported by Ascierto (2018).2\\nThis may lead to inaccurate estimates of power and'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='ings may largely be resolved by data centers reporting\\nan average PUE instead of a minimum observed value.\\nIn our work, we use a PUE of1.58, the global average\\nfor data centers in 2018 as reported by Ascierto (2018).2\\nThis may lead to inaccurate estimates of power and\\nenergy consumption of compute in energy-efÔ¨Åcient\\ndata centers; e.g., Google reports a Ô¨Çeetwide PUE of\\n1.10 for 2020.3 Future work may, therefore, explore\\nalternatives to using an average PUE value as well\\nas to include more components in our measurements\\nto improve the accuracy of our estimation. Currently,\\nthe software needed to take power measurements for\\ncomponents beyond the GPU, CPU, and DRAM is\\nextremely limited or in most cases, non-existent.\\nA.2. On the Topic of Carbon Offsetting\\nCarbon emissions can be compensated for by carbon\\noffsetting or the purchases of Renewable Energy\\nCredits (RECs). Carbon offsetting is the reduction in\\nemissions made to compensate for emissions occurring\\nelsewhere (Goodward & Kelly, 2010). We ignore such\\noffsets and RECs in our reporting as to encourage\\nresponsible computing in the ML community that\\nwould further reduce global emissions. See Henderson\\net al. (2020) for an extended discussion on carbon\\noffsets and why they also do not account for them\\nin the experiment-impact-tracker framework. Our\\ncarbon footprint estimate is solely based on the energy\\nconsumed during training of DL models.\\nA.3. Power and Energy Tracking\\nThe power and energy tracking in carbontracker'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='offsets and why they also do not account for them\\nin the experiment-impact-tracker framework. Our\\ncarbon footprint estimate is solely based on the energy\\nconsumed during training of DL models.\\nA.3. Power and Energy Tracking\\nThe power and energy tracking in carbontracker\\noccurs in the carbontracker thread. The thread\\ncontinuously collects instantaneous power samples\\nin real-time for every available device of the speciÔ¨Åed\\ncomponents. Once samples for every device has been\\ncollected, the thread will sleep for a Ô¨Åxed interval\\nbefore collecting samples again. When the epoch ends,\\nthe thread stores the epoch duration. Finally, after the\\ntraining loop completes we calculate the total energy\\n2Early versions (v1.1.2 and earlier) ofcarbontracker used\\na PUE of 1.58. This has subsequently been updated to1.67,\\nthe global average for 2019 (Ascierto, 2019).\\n3See https://www.google.com/about/\\ndatacenters/efficiency/'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='Carbontracker\\nFigure 6.A visualization of thecarbontracker control Ô¨Çow. Themain thread instantiates theCarbonTracker class which then\\nspawns the carbontracker and carbonintensity daemon threads. The carbontracker thread continuously collects\\npower measurements for available devices. Thecarbonintensity thread fetches the current carbon intensity every900 s.\\nWhen the speciÔ¨Åed epochs before predicting have passed, the total predicted consumption is reported tostdout and optional\\nlog Ô¨Åles. Similarly, when the speciÔ¨Åed amount of epochs have been monitored, the actual measured consumption is reported,\\nafter which thecarbontracker and carbonintensity threads join themain thread. Finally, thecarbontracker object runs\\nthe cleanup routinedelete() which releases all used resources.\\nconsumption E as\\nE =PUE\\n‚àë\\ne‚ààE\\n‚àë\\nd‚ààD\\nPavg,deTe (2)\\nwhere Pavg,de is the average power consumed by device\\nd‚ààDin epoch e‚ààE, and Te is the duration of epoche.\\nThe components supported by carbontracker in its\\ncurrent form are the GPU, CPU, and DRAM due to the\\naforementioned restrictions. NVIDIA GPUs represent\\na large share of Infrastructure-as-a-Service compute\\ninstance types with dedicated accelerators. So we\\nsupport NVIDIA GPUs as power sampling is exposed\\nthrough the NVIDIA Management Library (NVML)4.\\nLikewise, we support Intel CPUs and DRAM through\\nthe Intel Running Average Power Limit (Intel RAPL)\\ninterface (David et al., 2010).\\n4https://developer.nvidia.com/\\nnvidia-management-library-nvml\\nA.4. Converting'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='through the NVIDIA Management Library (NVML)4.\\nLikewise, we support Intel CPUs and DRAM through\\nthe Intel Running Average Power Limit (Intel RAPL)\\ninterface (David et al., 2010).\\n4https://developer.nvidia.com/\\nnvidia-management-library-nvml\\nA.4. Converting\\nEnergy Consumption to Carbon Emissions\\nWe can estimate the carbon emissions resulting from\\nthe electricity production of the energy consumed\\nduring training as the product of the energy and carbon\\nintensity as shown in (3):\\nCarbon Footprint= Energy Consumption√ó\\nCarbon Intensity. (3)\\nThe used carbon intensity heavily inÔ¨Çuences the\\naccuracy of this estimate. Incarbontracker, we support\\nthe fetching of carbon intensity in real-time through\\nexternal APIs.We dynamically determine the loca-\\ntion based on the IP address of the local compute\\nthrough the Python geocoding library geocoder5.\\nUnfortunately, there does not currently exist a globally\\naccurate, free, and publicly available real-time carbon\\nintensity database. This makes determining the carbon\\nintensity to use for the conversion more difÔ¨Åcult.\\n5https://github.com/DenisCarriere/\\ngeocoder'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='Carbontracker\\nWe solve this problem by using several APIs that are\\nlocal to each region. It is currently limited to Denmark\\nand Great Britain. Other regions default to an average\\ncarbon intensity for the EU-28 countries in 20176. For\\nDenmark we use data from Energi Data Service7 and\\nfor Great Britain we use the Carbon Intensity API8.\\nA.5. Logging\\nFinally,carbontracker has extensive logging capabilities\\nenabling transparency of measurements and enhanc-\\ning the reproducibility of experiments. The user may\\nspecify the desired path for these log Ô¨Åles. We use the\\nlogging API9 provided by the standard Python library.\\nAdditional functionality for interaction\\nwith logs has also been added through the\\ncarbontracker.parser module. Logs may\\neasily be parsed into Python dictionaries containing all\\ninformation regarding the training sessions, including\\npower and energy usages, epoch durations, devices\\nmonitored, whether the model stopped early, and the\\noutputted prediction. We further support aggregating\\nlogs into a single estimate of the total impact of all\\ntraining sessions. By using different log directories, the\\nuser can easily keep track of the total impact of each\\nmodel trained and developed. The user may then use\\nthe provided parser functionality to estimate the full\\nimpact of R&D.\\nCurrent version of carbontracker uses kilometers trav-\\nelled by car as the carbon emissions conversion. This\\ndata is retrieved from the averageCO2eq emissions of'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='model trained and developed. The user may then use\\nthe provided parser functionality to estimate the full\\nimpact of R&D.\\nCurrent version of carbontracker uses kilometers trav-\\nelled by car as the carbon emissions conversion. This\\ndata is retrieved from the averageCO2eq emissions of\\na newly registered car in the European Union in 201810.\\nB. Models and Data\\nIn our experimental evaluation, we trained two CNN\\nmodels on three medical image datasets for the task\\nof image segmentation. The models were developed\\nin PyTorch (Paszke et al., 2019). We describe each of\\nthe models and datasets in turn below.\\nU-net DRIVE This is a standard U-net model (Ron-\\nneberger et al., 2015) trained on the DRIVE dataset\\n6https://www.eea.europa.\\neu/data-and-maps/data/\\nco2-intensity-of-electricity-generation\\n7https://energidataservice.dk/\\n8https://carbonintensity.org.uk/\\n9https://docs.python.org/3/library/\\nlogging.html\\n10https://www.eea.europa.\\neu/data-and-maps/indicators/\\naverage-co2-emissions-from-motor-vehicles/\\nassessment-1\\n(Staal et al., 2004). DRIVE stands for Digital Retinal\\nImages for Vessel Extraction and is intended for\\nsegmentation of blood vessels in retinal images. The\\nimages are 768 by 584 pixels and JPEG compressed.\\nWe used a training set of15 images and trained for300\\nepochs with a batch size of4 and a learning rate of10‚àí3\\nwith the Adam optimizer (Kingma & Ba, 2014).\\nU-net CXR The model is based on a U-net (Ron-\\nneberger et al., 2015) with slightly changed parameters.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='We used a training set of15 images and trained for300\\nepochs with a batch size of4 and a learning rate of10‚àí3\\nwith the Adam optimizer (Kingma & Ba, 2014).\\nU-net CXR The model is based on a U-net (Ron-\\nneberger et al., 2015) with slightly changed parameters.\\nThe dataset comprises of chest X-rays (CXR) with lung\\nmasks curated for pulmonary tuberculosis detection\\n(Jaeger et al., 2014). We use528 CXRs for training and\\n176 for validation without any data augmentation. We\\ntrained the model for 200 epochs with a batch size of\\n12, a learning rate of 10‚àí4, and weight decay of 10‚àí5\\nwith the Adam optimizer (Kingma & Ba, 2014).\\nU-net LIDC This is also a standard U-net model\\n(Ronneberger et al., 2015) but trained on a preprocessed\\nLIDC-IDRI dataset (Armato III et al., 2004) 11. The\\nLIDC-IDRI dataset consists of 1018 thoracic computed\\ntomography (CT) scans with annotated lesions from\\nfour different radiologists. We trained our model on\\nthe annotations of a single radiologist for100 epochs.\\nWe used a batch size of64 and a learning rate of10‚àí3\\nwith the Adam optimizer (Kingma & Ba, 2014).\\nlungV AE CXRThis model and dataset is from the\\nopen source model available from Selvan et al. (2020).\\nThe model uses a U-net type segmentation network\\nand a variational encoder for data imputation. The\\ndataset is the same CXR dataset as used in our above\\nU-net CXR model. 528 CXRs are used for training and\\n176 for validation. The model was trained with a batch'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='The model uses a U-net type segmentation network\\nand a variational encoder for data imputation. The\\ndataset is the same CXR dataset as used in our above\\nU-net CXR model. 528 CXRs are used for training and\\n176 for validation. The model was trained with a batch\\nsize of 12 and a learning rate of 10‚àí4 with the Adam\\noptimizer (Kingma & Ba, 2014) for a maximum of200\\nepoch using early stopping based on the validation loss.\\nThe Ô¨Årst run was90 epochs, and the second run was97.\\nC. Additional Experiments\\nC.1. Performance Impact of Carbontracker\\nThe performance impact of usingcarbontracker to moni-\\ntor all training epochs is shown as a boxplot in Figure 7.\\nWe see that the mean increase in epoch duration for our\\nU-net models across two runs is0.19% on DRIVE,1.06%\\non LIDC, and‚àí0.58% on CXR. While we see individual\\nepochs with a relative increase of up to5% on LIDC and\\neven 22% on DRIVE, this is more likely attributed to\\nthe stochasticity in epoch duration than tocarbontracker.\\nWe further note that the Ô¨Çuctuations in epoch dura-\\n11https://github.com/stefanknegt/\\nProbabilistic-Unet-Pytorch'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='Carbontracker\\nFigure 7.Box plot of the performance impact (%) of using\\ncarbontracker to monitor all training epochs shown as the\\nrelative increase in epoch duration compared to a baseline\\nwithout carbontracker. The whiskers and outliers are obtained\\nfrom the Tukey method using 1.5 times IQR.\\ntion (Figure 7) are not caused bycarbontracker. These\\nÔ¨Çuctuations are also witnessed in the baseline runs.\\nD. Estimating the\\nEnergy and Carbon Footprint of GPT-3\\nBrown et al. (2020) report that the GPT-3 model with\\n175 billion parameters used 3.14 ¬∑1023 Ô¨Çoating point\\noperations (FPOs) of compute to train using NVIDIA\\nV100 GPUs on a cluster provided by Microsoft. We\\nassume that these are the most powerful V100 GPUs,\\nthe V100S PCIe model, with a tensor performance of\\n130 TFLOPS12 and that the Microsoft data center has\\na PUE of 1.125, the average for new Microsoft data\\ncenters in 201513. The compute time on a single GPU is\\ntherefore\\n3.14¬∑1023 FPOs\\n130¬∑1012 FLOPS =2415384615.38s=27955 .84d.\\nThis is equivalent to about310 GPUs running non-stop\\nfor 90 days. If we use the thermal design power (TDP)\\nof the V100s and the PUE, we can estimate that this\\nused\\n250W¬∑2415384615.38s¬∑1.125=679326923075 .63J\\n=188701.92kWh.\\n12https://bit.ly/2zFsOK2\\n13http://download.microsoft.com/download/\\n8/2/9/8297f7c7-ae81-4e99-b1db-d65a01f7a8ef/\\nmicrosoft_cloud_infrastructure_datacenter_\\nand_network_fact_sheet.pdf\\nCarbon intensity\\n(gCO2eq/kWh)\\n13\\n819\\nFigure 8.Average carbon intensity ( gCO2eq/kWh) of'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='=188701.92kWh.\\n12https://bit.ly/2zFsOK2\\n13http://download.microsoft.com/download/\\n8/2/9/8297f7c7-ae81-4e99-b1db-d65a01f7a8ef/\\nmicrosoft_cloud_infrastructure_datacenter_\\nand_network_fact_sheet.pdf\\nCarbon intensity\\n(gCO2eq/kWh)\\n13\\n819\\nFigure 8.Average carbon intensity ( gCO2eq/kWh) of\\nEU-28 countries in 2016. The intensity is calculated as\\nthe ratio of emissions from public electricity produc-\\ntion and gross electricity production. Data is provided\\nby the European Environment Agency (EEA). See\\nhttps://www.eea.europa.eu/ds_resolveuid/\\n3f6dc9e9e92b45b9b829152c4e0e7ade.\\nUsing the average carbon intensity of USA in 2017 of\\n449.06 gCO2eq/kWh14, we see this may emit up to\\n449.06gCO2eq/kWh¬∑188701.92kWh\\n=84738484.20gCO2eq\\n=84738.48kgCO2eq.\\nThis is equivalent to\\n84738484.20gCO2eq\\n120.4gCO2eqkm‚àí1 =703808.01km\\ntravelled by car using the averageCO2eq emissions of\\na newly registered car in the European Union in 201815.\\n14https://www.eia.gov/tools/faqs/faq.php?\\nid=74&t=11\\n15https://www.eea.europa.\\neu/data-and-maps/indicators/\\naverage-co2-emissions-from-motor-vehicles/\\nassessment-1'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-08T00:39:00+00:00', 'author': '', 'keywords': '', 'moddate': '2020-07-08T00:39:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/2007.03051v1.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='Carbontracker\\nFigure 9.Comparison of predicted and measured values of energy (kWh) and duration (s) per epoch when predicting after a\\nsingle epoch. (row 1) Energy. (row 2) Cumulative energy. (row 3) Duration. (row 4) Cumulative duration. Each column shows\\na different model and dataset, as detailed in Appendix B. The initial epoch is often characterized by low energy usage and\\nshort epoch duration compared to the following epochs. Notwithstanding, the linear prediction model used bycarbontracker\\nmay still lead to reasonable predictions after monitoring a single epoch.')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store():\n",
        "    # Load and preprocess the document\n",
        "    raw_docs = load_scientific_paper(config.DOCUMENT_PATH)\n",
        "    if not raw_docs:\n",
        "        raise ValueError(\"Nenhum conte√∫do v√°lido encontrado no documento\")\n",
        "\n",
        "    # Split text into manageable chunks\n",
        "    split_docs = split_scientific_text(raw_docs)\n",
        "\n",
        "    # Create embeddings and initialize the vector store\n",
        "    vector_store = Chroma.from_documents(\n",
        "        documents=split_docs,\n",
        "        embedding=OpenAIEmbeddings(\n",
        "            openai_api_key=config.OPENAI_API_KEY,\n",
        "            model=\"text-embedding-3-large\",  # Best for scientific content\n",
        "            dimensions=3072\n",
        "        ),\n",
        "        persist_directory=config.CHROMA_DIR\n",
        "    )\n",
        "    vector_store.persist()\n",
        "    print(\"Number of documents in Chroma DB:\", vector_store._collection.count())\n",
        "\n",
        "    return vector_store\n",
        "\n",
        "create_vector_store()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVyWdaLzagDk",
        "outputId": "ff9653de-f0cc-459f-a273-46b9dd39270c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 11 documents.\n",
            "Preview of first document:\n",
            "Carbontracker: Tracking and Predicting the Carbon Footprint of Training\n",
            "Deep Learning Models\n",
            "Lasse F. Wolff Anthony‚àó1 Benjamin Kanding‚àó1 Raghavendra Selvan1\n",
            "Abstract\n",
            "Deep learning (DL) can achieve impressive\n",
            "results across a wide variety of tasks, but\n",
            "this often comes at the cost of training\n",
            "models for extensive periods on specialized\n",
            "hardware accelerators. This energy-intensive\n",
            "workload has seen immense growth in recent\n",
            "years. Machine learning (ML) may become\n",
            "a signiÔ¨Åcant contributor to climate\n",
            "Document split into 36 chunks.\n",
            "Number of documents in Chroma DB: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-86f2986ce9ff>:20: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vector_store.persist()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x7e32a06b80d0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_rag_system():\n",
        "    # Set up the embedding model\n",
        "    embedding_model = OpenAIEmbeddings(\n",
        "        openai_api_key=config.OPENAI_API_KEY,\n",
        "        model=\"text-embedding-3-large\",\n",
        "        dimensions=3072\n",
        "    )\n",
        "\n",
        "    # Either load an existing vector store or create a new one\n",
        "    if os.path.exists(config.CHROMA_DIR):\n",
        "        vector_store = Chroma(\n",
        "            persist_directory=config.CHROMA_DIR,\n",
        "            embedding_function=embedding_model\n",
        "        )\n",
        "    else:\n",
        "        vector_store = create_vector_store()\n",
        "\n",
        "    # Adjust the retriever: lower threshold and retrieve more chunks for richer context\n",
        "    retriever = vector_store.as_retriever(\n",
        "        search_type=\"mmr\",  # Maximal Marginal Relevance for diversity\n",
        "        search_kwargs={\n",
        "            \"k\": 10,               # Retrieve more chunks\n",
        "            #\"score_threshold\": 0.3  # Lower threshold to include more context\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Use an English prompt for consistency with your queries\n",
        "    SCIENCE_PROMPT = ChatPromptTemplate.from_template(\n",
        "        \"\"\"You are a research assistant specialized in analyzing scientific papers.\n",
        "Use the provided context strictly to answer the question.\n",
        "\n",
        "Article context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Required format:\n",
        "- Provide a concise answer (maximum 300 words)\n",
        "- Use technical terms in English (if applicable) in parentheses\n",
        "- Cite the section/page of the document if applicable\n",
        "- If there is insufficient information, say \"Insufficient information in the article\"\n",
        "\n",
        "Answer:\"\"\"\n",
        "    )\n",
        "\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_key=config.OPENAI_API_KEY,\n",
        "        model_name=config.MODEL_NAME,\n",
        "        temperature=0.1  # Less creativity, more factual\n",
        "    )\n",
        "\n",
        "    # Build the RAG chain\n",
        "    return (\n",
        "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "        | SCIENCE_PROMPT\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "initialize_rag_system()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdHquugtbW6S",
        "outputId": "c5eb0cf8-ac8f-4229-a6a4-64e6a7e29a44"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              "  context: VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7e329fa1b0d0>, search_type='mmr', search_kwargs={'k': 10}),\n",
              "  question: RunnablePassthrough()\n",
              "}\n",
              "| ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='You are a research assistant specialized in analyzing scientific papers.\\nUse the provided context strictly to answer the question.\\n\\nArticle context:\\n{context}\\n\\nQuestion: {question}\\n\\nRequired format:\\n- Provide a concise answer (maximum 300 words)\\n- Use technical terms in English (if applicable) in parentheses\\n- Cite the section/page of the document if applicable\\n- If there is insufficient information, say \"Insufficient information in the article\"\\n\\nAnswer:'), additional_kwargs={})])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e329fa1f910>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e32aa86f8d0>, root_client=<openai.OpenAI object at 0x7e32a08402d0>, root_async_client=<openai.AsyncOpenAI object at 0x7e329fa1fd10>, model_name='gpt-4-1106-preview', temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
              "| StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scientific_chat():\n",
        "    rag_chain = initialize_rag_system()\n",
        "    print(\"Scientific Analysis System Ready. Ask your question about the article (in English):\")\n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"\\nQuestion: \")\n",
        "            if query.lower() in [\"exit\", \"quit\"]:\n",
        "                break\n",
        "            response = rag_chain.invoke(query)\n",
        "            print(f\"\\nAnswer:\\n{response}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            print(\"Please rephrase your question or try again.\")\n",
        "\n",
        "scientific_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkoQY1E6b2iL",
        "outputId": "bc169dc6-ea92-482b-b346-bb1cdd5efe61"
      },
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scientific Analysis System Ready. Ask your question about the article (in English):\n",
            "\n",
            "Question: what is the main discovery in this paper?\n",
            "\n",
            "Answer:\n",
            "The main discovery in this paper is the development and implementation of \"Carbontracker,\" an open-source tool written in Python designed to track and predict the energy consumption and carbon emissions associated with training deep learning (DL) models. The tool aims to raise awareness about the environmental impact of the increasing computational demands in DL by providing accurate reporting of energy and carbon footprints. Carbontracker operates as a multithreaded program, allowing it to collect power measurements and fetch real-time carbon intensity data without disrupting the main training process of the model.\n",
            "\n",
            "The paper also discusses the effectiveness of power management techniques like dynamic voltage and frequency scaling (DVFS) in conserving energy consumption during the training and inference of different deep neural networks (DNNs). It highlights that default frequency settings on GPUs, such as NVIDIA‚Äôs Pascal P100 and Volta V100, are often not optimized for energy efficiency in DNN training and inference.\n",
            "\n",
            "Furthermore, the paper presents an analysis of the environmental impact of training large-scale models, such as GPT-3, which requires significant computational resources (estimated at 28000 GPU-days excluding R&D). The authors advocate for the inclusion of energy efficiency as a metric in evaluating DL models, alongside traditional performance metrics like accuracy and area under the curve (AUC) score.\n",
            "\n",
            "The discovery is discussed in the sections \"5. Discussion and Conclusion\" on page 3 (page label 4) and \"Carbontracker\" on page 5 (page label 6) of the document. The effectiveness of DVFS is mentioned on page 3 (page label 4), and the environmental impact of training large-scale models is also discussed on page 3 (page label 4). The principles behind Carbontracker are outlined on page 1 (page label 2).\n",
            "\n",
            "Question: what are the main concerns about tracking carbon footprint and LLMs?\n",
            "\n",
            "Answer:\n",
            "The main concerns about tracking the carbon footprint of Large Language Models (LLMs) like GPT-3 are related to the significant energy consumption and subsequent carbon emissions associated with training these models on specialized hardware accelerators. The article emphasizes that while Deep Learning (DL) can achieve impressive results, it often comes at the cost of extensive training periods on hardware that is energy-intensive. This workload has seen immense growth and could become a significant contributor to climate change if the trend continues (Abstract, page 1).\n",
            "\n",
            "The authors propose that practitioners should be aware of their energy and carbon footprint and actively take steps to reduce it. They introduce \"carbontracker,\" a tool for tracking and predicting the energy and carbon footprint of training DL models, to enable practitioners to be informed and take action to reduce their footprint (Abstract, page 1).\n",
            "\n",
            "The article suggests that the total energy and carbon footprint of model development and training should be reported alongside traditional performance metrics like accuracy, to promote responsible computing in Machine Learning (ML) and research into energy-efficient Deep Neural Networks (DNNs) (page 3, section \"SOTA-results in DL\").\n",
            "\n",
            "Furthermore, the article discusses the importance of considering the carbon intensity of the region where the training takes place, as well as the time of day, since carbon intensity can vary significantly (page 3, section \"Training Times\"). For large-scale models trained on multiple GPUs for long periods, training in low carbon intensity regions is imperative to avoid several megagrams of carbon emissions (page 3, section \"Training Times\").\n",
            "\n",
            "In summary, the concerns are centered around the high energy consumption and carbon emissions of LLMs, the need for awareness and reporting of these impacts, and the promotion of strategies to mitigate the carbon footprint associated with DL model training (Abstract, page 1; page 3, sections \"SOTA-results in DL\" and \"Training Times\").\n",
            "\n",
            "Question: exit\n"
          ]
        }
      ]
    }
  ]
}